#!/usr/bin/env python3

"""
Copy a day's data from live ping tables to stable ping tables,
deduplicating on document_id.

By default, the script will process all tables in datasets named
like *_live, copying data into table of the same name in *_stable
datasets. The script can be configured to exclude a list of tables
or to process only a specific list of tables.
"""

from argparse import ArgumentParser
from datetime import datetime, timedelta
from fnmatch import fnmatch
from itertools import groupby
from multiprocessing.pool import ThreadPool
from uuid import uuid4

from google.cloud import bigquery

QUERY_DAY_BASE = """
  -- A single day of a live ping table.
  base AS (
  SELECT
    *
  FROM
    `{source_table_spec}`
  WHERE
    DATE(submission_timestamp) = @submission_date ),
"""

QUERY_HOUR_BASE = """
  -- Document_ids already seen in previous hours on this day
  previous_hours_document_ids AS (
  SELECT
    DISTINCT document_id
  FROM
    `{source_table_spec}`
  WHERE
    DATE(submission_timestamp) = DATE(@submission_hour)
    AND submission_timestamp < @submission_hour),
  -- A single hour of a live ping table.
  base AS (
  SELECT
    *
  FROM
    `{source_table_spec}`
  WHERE
    TIMESTAMP_TRUNC(submission_timestamp, HOUR) = @submission_hour
    AND document_id NOT IN (SELECT * FROM previous_hours_document_ids)),
"""

QUERY_TEMPLATE = """\
WITH
  {base}
  --
  -- We assume that the pipeline has already filtered out most duplicates,
  -- so a list of all document_ids that appear more than once should be
  -- reasonably small and fit in memory as a lookup table on each node.
  duplicate_ids AS (
  SELECT
    document_id
  FROM
    base
  GROUP BY
    document_id
  HAVING
    COUNT(document_id) > 1),
  --
  -- We can directly include all singly-occurring documents in the final
  -- result set without incurring the expense of sorting in a window function.
  singly_occurring_documents AS (
  SELECT
    base.*
  FROM
    base
  LEFT JOIN
    duplicate_ids
  USING
    (document_id)
  WHERE
    duplicate_ids.document_id IS NULL),
  --
  -- For documents that appear multiple times, we have to order them by
  -- assigning a row number.
  numbered_duplicates AS (
  SELECT
    base.*,
    ROW_NUMBER() OVER (PARTITION BY document_id ORDER BY submission_timestamp) AS _n
  FROM
    base
  JOIN
    duplicate_ids
  USING
    (document_id) ),
  --
  -- We retain only the first seen document for each ID, according to timestamp.
  deduplicated_documents AS (
  SELECT
    * EXCEPT (_n)
  FROM
    numbered_duplicates
  WHERE
    _n = 1 )
  --
SELECT * FROM singly_occurring_documents
UNION ALL
SELECT * FROM deduplicated_documents
"""

parser = ArgumentParser(description=__doc__)
parser.add_argument(
    "--project_id",
    "--project-id",
    default="moz-fx-data-shar-nonprod-efed",
    help="ID of the project in which to run query jobs",
)
parser.add_argument(
    "--date",
    required=True,
    type=lambda d: datetime.strptime(d, "%Y-%m-%d").date(),
    help="Which day's data to copy, in format 2019-01-01",
)
parser.add_argument(
    "--parallelism",
    default=4,
    type=int,
    help="Maximum number of queries to execute concurrently",
)
parser.add_argument(
    "--dry_run",
    "--dry-run",
    action="store_true",
    help=(
        "Do not run queries, but instead print the query job config "
        "and bytes that would be processed"
    ),
)
parser.add_argument(
    "--priority",
    default="INTERACTIVE",
    type=str.upper,
    choices=["BATCH", "INTERACTIVE"],
    help=(
        "Deduplicate one hour at a time, rather than for whole days at once; "
        "avoids memory overflow at the cost of less effective clustering; "
        "recommended only for tables failing due to memory overflow"
    ),
)
parser.add_argument(
    "--hourly",
    action="store_true",
    help=(
        "Deduplicate one hour at a time, rather than for whole days at once; "
        "avoids memory overflow at the cost of less effective clustering; "
        "recommended only for tables failing due to memory overflow"
    ),
)
group = parser.add_mutually_exclusive_group()
group.add_argument(
    "--only",
    nargs="+",
    dest="only_tables",
    help=(
        "Process only the given tables; "
        "pass names or globs like 'telemetry_live.main_v*' "
    ),
)
group.add_argument(
    "--except",
    nargs="+",
    dest="except_tables",
    help=(
        "Process all tables in *_live datasets except for the given tables; "
        "pass names or globs like 'telemetry_live.main_v*'"
    ),
)

temporary_dataset = None


def get_temporary_dataset(client):
    """Get a cached reference to the dataset used for server-assigned destinations."""
    global temporary_dataset
    if temporary_dataset is None:
        # look up the dataset used for query results without a destination
        dry_run = bigquery.QueryJobConfig(dry_run=True)
        destination = client.query("SELECT 1", dry_run).destination
        temporary_dataset = client.dataset(destination.dataset_id, destination.project)
    return temporary_dataset


def get_temporary_table(client, schema, clustering_fields, time_partitioning, date, dry_run):
    """Generate a temporary table and return the specified date partition.

    Generates a table name that looks similar to, but won't collide with, a
    server-assigned table and that the web console will consider temporary.
    Table expiration can't be set from a query job, so the table is created
    here.

    In order for query results to use time partitioning, and by extension
    clustering, destination table must be explicitly set. Destination must be
    generated locally and never collide with server-assigned table names,
    because server-assigned tables cannot be modified. Server-assigned tables
    for a dry_run query cannot be detected by client.list_tables and cannot be
    reused as that consitutes a modification.

    Server-assigned tables have names that start with "anon" and follow with
    either 40 hex characters or a uuid replacing "-" with "_", and cannot be
    modified (i.e. reused).

    The web console considers a table temporary if the dataset name starts with
    "_" and table_id starts with "anon" and is followed by at least one
    character.
    """
    dataset = get_temporary_dataset(client)
    table = bigquery.Table(dataset.table(f"anon{uuid4().hex}"), schema)
    if not dry_run:
        table.expires = datetime.now() + timedelta(days=1)
        table.clustering_fields = clustering_fields
        table.time_partitioning = time_partitioning
        table = client.create_table(table, exists_ok=False)
    return f"{sql_full_table_id(table)}${date:%Y%m%d}"


def sql_full_table_id(table):
    return f"{table.project}.{table.dataset_id}.{table.table_id}"


def get_deduplication_query(live_table, hourly):
    base_template = QUERY_HOUR_BASE if hourly else QUERY_DAY_BASE
    base = base_template.format(source_table_spec=sql_full_table_id(live_table))
    return QUERY_TEMPLATE.format(base=base.strip())


def get_query_job_configs(client, stable_table, date, dry_run, hourly, priority):
    kwargs = dict(use_legacy_sql=False, dry_run=dry_run, priority=priority)
    if hourly:
        stable_table = client.get_table(stable_table)
        for hour in range(24):
            value = (datetime(*date.timetuple()[:6]) + timedelta(hours=hour))
            yield bigquery.QueryJobConfig(
                destination=get_temporary_table(
                    client=client,
                    schema=stable_table.schema,
                    clustering_fields=stable_table.clustering_fields,
                    time_partitioning=stable_table.time_partitioning,
                    date=date,
                    dry_run=dry_run,
                ),
                query_parameters=[
                    bigquery.ScalarQueryParameter("submission_hour", "TIMESTAMP", value)
                ],
                **kwargs,
            )
    else:
        yield bigquery.QueryJobConfig(
            destination=stable_table,
            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
            query_parameters=[
                bigquery.ScalarQueryParameter("submission_date", "DATE", date)
            ],
            **kwargs,
        )


def run_deduplication_query(client, sql, stable_table, job_config):
    query_job = client.query(sql, job_config)
    if not query_job.dry_run:
        query_job.result()
    return stable_table, query_job


def copy_join_parts(client, stable_table, query_jobs):
    total_bytes = sum(query.total_bytes_processed for query in query_jobs)
    if query_jobs[0].dry_run:
        api_repr = query_jobs[0].to_api_repr()
        if len(query_jobs) > 1:
            print(f"Would process {total_bytes} bytes: [{api_repr},...]")
            print(f"Would copy {len(query_jobs)} results to populate {stable_table}")
        else:
            print(f"Would process {total_bytes} bytes: {api_repr}")
    else:
        print(f"Processed {total_bytes} bytes to populate {stable_table}")
        if len(query_jobs) > 1:
            sources = [job.destination for job in query_jobs]
            job_config = bigquery.CopyJobConfig(
                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE
            )
            copy_job = client.copy_table(sources, stable_table, job_config=job_config)
            copy_job.result()
            print(f"Copied {len(query_jobs)} results to populate {stable_table}")
            for source in sources:
                client.delete_table(sql_full_table_id(source).split("$")[0])
            print(f"Deleted {len(query_jobs)} temporary tables")


def main():
    args = parser.parse_args()

    client = bigquery.Client()
    live_datasets = [
        d
        for d in client.list_datasets(args.project_id)
        if d.dataset_id.endswith("_live")
    ]

    job_args = []

    for live_dataset in live_datasets:
        stable_dataset_id = live_dataset.dataset_id[:-5] + "_stable"
        stable_dataset = client.dataset(stable_dataset_id, args.project_id)
        for live_table in client.list_tables(live_dataset.reference):
            live_table_id = live_table.table_id
            live_table_spec = f"{live_table.dataset_id}.{live_table_id}"
            stable_table = stable_dataset.table(f"{live_table_id}${args.date:%Y%m%d}")
            if args.except_tables is not None and any(
                fnmatch(live_table_spec, pattern) for pattern in args.except_tables
            ):
                print(f"Skipping {live_table_spec} due to --except argument")
                continue
            if args.only_tables is not None and not any(
                fnmatch(live_table_spec, pattern) for pattern in args.only_tables
            ):
                print(f"Skipping {live_table_spec} due to --only argument")
                continue
            sql = get_deduplication_query(live_table, args.hourly)
            job_args.extend(
                (client, sql, stable_table, job_config)
                for job_config in get_query_job_configs(
                    client,
                    stable_table,
                    args.date,
                    args.dry_run,
                    args.hourly,
                    args.priority,
                )
            )

    with ThreadPool(args.parallelism) as pool:
        # preserve job_args order so results stay sorted by stable_table for groupby
        results = pool.starmap(run_deduplication_query, job_args, chunksize=1)
        copy_args = [
            (client, stable_table, [query_job for _, query_job in group])
            for stable_table, group in groupby(results, key=lambda result: result[0])
        ]
        pool.starmap(copy_join_parts, copy_args)


if __name__ == "__main__":
    main()
