name: Build, Test, and Deploy
on:
  push:
    branches:
  pull_request:
    types:
      - opened
  merge_group:


env:
  PYTHON_VERSION: '3.11'

jobs:
  decide-runs:
    name: Decide what checks to run based on changed files
    runs-on: ubuntu-latest
    outputs:
      validate-routines: ${{ steps.set-flags.outputs.validate-routines }}
      deploy: ${{ steps.set-flags.outputs.deploy }}
      validate-sql: ${{ steps.set-flags.outputs.validate-sql }}
      validate-bqetl: ${{ steps.set-flags.outputs.validate-bqetl }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
          fetch-depth: 0
      - id: changed-files-routines
        uses: tj-actions/changed-files@v47
        with:
          files: |
            routines/**
            .github/workflows/**
          base_sha: ${{ github.event.pull_request.base.sha || github.event.before || 'main' }}
      - id: changed-files-sql
        uses: tj-actions/changed-files@v47
        with:
          files: |
            sql/**
            sql_generators/**
            dags.yaml
            .github/workflows/**
          base_sha: ${{ github.event.pull_request.base.sha || github.event.before || 'main' }}
      - id: changed-files-bqetl
        uses: tj-actions/changed-files@v47
        with:
          files: |
            bigquery_etl/**
            script/bqetl
            script/entrypoint
            requirements.txt
            requirements.in
            .github/workflows/**
          base_sha: ${{ github.event.pull_request.base.sha || github.event.before || 'main' }}
      - name: Decide which jobs to run
        id: set-flags
        run: |
          echo "validate-routines=${{ steps.changed-files-routines.outputs.any_changed }}" >> $GITHUB_OUTPUT
          echo "validate-sql=${{ steps.changed-files-sql.outputs.any_changed }}" >> $GITHUB_OUTPUT
          echo "validate-bqetl=${{ steps.changed-files-bqetl.outputs.any_changed }}" >> $GITHUB_OUTPUT
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "deploy=true" >> $GITHUB_OUTPUT
          else
            echo "deploy=false" >> $GITHUB_OUTPUT
          fi

  build:
    name: Build Environment
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Build
        run: |
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --no-deps -r requirements.txt
          pip-sync --pip-args=--no-deps
      - name: Save cached virtualenv
        if: steps.cache-venv.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv

  verify-requirements:
    name: Verify Requirements
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Install pip-tools
        run: pip install pip-tools
      - name: Verify that requirements.txt contains the right dependencies for this python version
        run: |
          pip-compile --allow-unsafe --generate-hashes --quiet
          git diff --exit-code -G '^ *[^# ]' -- requirements.txt

  verify-format-yaml:
    name: Verify YAML Format
    runs-on: ubuntu-latest
    needs: [build]
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Yamllint Test
        run: PATH=".venv/bin:$PATH" yamllint -c .yamllint.yaml .

  verify-format-sql:
    name: Verify SQL Format
    runs-on: ubuntu-latest
    needs: [build, decide-runs]
    if: needs.decide-runs.outputs.validate-sql == 'true' || needs.decide-runs.outputs.deploy == 'true'
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Verify that SQL is correctly formatted
        run: |
          PATH=".venv/bin:$PATH" script/bqetl format --check \
          $(git ls-tree -d HEAD --name-only)

  test-bqetl:
    name: Test bqetl
    runs-on: ubuntu-latest
    needs: [build, decide-runs]
    if: needs.decide-runs.outputs.validate-bqetl == 'true' || needs.decide-runs.outputs.deploy == 'true'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: PyTest with linters
        run: |
          PATH=".venv/bin:$PATH" pytest --black --flake8 \
            --isort --mypy-ignore-missing-imports --pydocstyle \
            -m "not (routine or sql or integration)" \
            -p no:bigquery_etl.pytest_plugin.routine \
            -p no:bigquery_etl.pytest_plugin.sql \
            -n 8

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [build, decide-runs]
    if: needs.decide-runs.outputs.validate-bqetl == 'true' || needs.decide-runs.outputs.deploy == 'true'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_STAGE_SERVICE_ACCOUNT_EMAIL }}
      - name: Set project ID for integration tests
        run: echo "GOOGLE_PROJECT_ID=${{ steps.auth.outputs.project_id }}" >> $GITHUB_ENV
      - name: PyTest Integration Test
        timeout-minutes: 30
        run: |
          PATH=".venv/bin:$PATH" script/entrypoint -m 'integration' -n 8 \
          -p no:bigquery_etl.pytest_plugin.routine \
          -p no:bigquery_etl.pytest_plugin.sql

  generate-sql:
    name: Generate SQL
    runs-on: ubuntu-latest
    needs: [build, decide-runs]
    if: needs.decide-runs.outputs.validate-sql == 'true' || needs.decide-runs.outputs.validate-routines == 'true' || needs.decide-runs.outputs.deploy == 'true'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: true
          fetch-depth: 0
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: Generate SQL content
        run: |
          # Fetch main branch for merge-base
          git fetch origin main:main --no-tags
          MAIN_MERGE_BASE=$(git merge-base main HEAD || true)
          echo "merge base with main: $MAIN_MERGE_BASE"

          git clone -b generated-sql https://github.com/mozilla/bigquery-etl ~/remote-generated-sql
          cd ~/remote-generated-sql

          if [[ -n $MAIN_MERGE_BASE ]] && git show-ref --tags c-${MAIN_MERGE_BASE:0:9} --quiet; then
            echo "Tag c-${MAIN_MERGE_BASE:0:9} exists in generated sql"
            git checkout tags/c-${MAIN_MERGE_BASE:0:9}
          else
            export LATEST_TAG_GENERATED_SQL=`git describe --tags --abbrev=0`
            LATEST_TAG_GENERATED_SQL=${LATEST_TAG_GENERATED_SQL/c-/""}
            echo "Latest tag on generated-sql branch: c-$LATEST_TAG_GENERATED_SQL"
          fi

          cd ~
          git clone -b main https://github.com/mozilla/bigquery-etl ~/remote-bigquery-etl-main
          cd ~/remote-bigquery-etl-main
          export LATEST_COMMIT_ON_MAIN=`git rev-parse --short HEAD`
          git checkout $LATEST_TAG_GENERATED_SQL
          echo "Latest commit on main: $LATEST_COMMIT_ON_MAIN"

          if [[ -n $MAIN_MERGE_BASE ]]; then
            echo "Checking out $MAIN_MERGE_BASE for diff"
            git checkout $MAIN_MERGE_BASE
          fi

          cd $GITHUB_WORKSPACE
          export GENERATED_SQL_CHANGED=false

          if [ "${{ github.event.inputs.trigger-sql-generation }}" = "true" ]; then
            echo "trigger-sql-generation was set to true; run SQL generation"
            GENERATED_SQL_CHANGED=true
          fi

          if [ "${{ github.ref_name }}" = main ]; then
            echo "On main branch; generate SQL"
            GENERATED_SQL_CHANGED=true
          fi

          if [[ $(git diff --no-index --name-only --diff-filter=DR ~/remote-bigquery-etl-main/sql sql) ]]; then
            echo "SQL file was removed; re-generate SQL"
            GENERATED_SQL_CHANGED=true
          fi

          if [[ $(diff -qr --no-dereference sql_generators ~/remote-bigquery-etl-main/sql_generators) ]]; then
            echo "SQL generators changed; re-generate SQL"
            GENERATED_SQL_CHANGED=true
          fi

          mkdir -p /tmp/workspace/generated-sql
          cp bqetl_project.yaml /tmp/workspace/generated-sql/

          if [ "$GENERATED_SQL_CHANGED" = "true" ]; then
            echo "Changes made will affect generated SQL. Run SQL generators."

            cp -r sql/ /tmp/workspace/generated-sql/sql
            PATH=".venv/bin:$PATH" script/bqetl generate all \
              --ignore derived_view_schemas \
              --output-dir /tmp/workspace/generated-sql/sql/ \
              --target-project moz-fx-data-shared-prod
            PATH=".venv/bin:$PATH" script/bqetl format /tmp/workspace/generated-sql/sql/
          else
            echo "Changes made don't affect generated SQL. Use content from generated-sql"

            cp -r ~/remote-generated-sql/sql/ /tmp/workspace/generated-sql/sql
            cp -a sql/. /tmp/workspace/generated-sql/sql
          fi

          PATH=".venv/bin:$PATH" script/bqetl dependency record \
            --skip-existing \
            "/tmp/workspace/generated-sql/sql/"
          PATH=".venv/bin:$PATH" script/bqetl metadata update \
            --sql-dir /tmp/workspace/generated-sql/sql/ \
            /tmp/workspace/generated-sql/sql/
          PATH=".venv/bin:$PATH" script/bqetl monitoring update /tmp/workspace/generated-sql/sql/
      - name: Copy schema cache for reuse in other jobs
        run: |
          mkdir -p /tmp/workspace/schema-cache
          if [ -d /tmp/bigquery_etl_schemas ]; then
            cp -r /tmp/bigquery_etl_schemas /tmp/workspace/schema-cache/
          fi
      - uses: actions/upload-artifact@v4
        with:
          name: generated-sql
          path: /tmp/workspace/generated-sql
      - uses: actions/upload-artifact@v4
        with:
          name: schema-cache
          path: /tmp/workspace/schema-cache

  deploy-changes-to-stage:
    name: Deploy Changes to Stage
    runs-on: ubuntu-latest
    needs: [generate-sql]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql
          path: /tmp/workspace/generated-sql
      - uses: actions/download-artifact@v4
        with:
          name: schema-cache
          path: /tmp/workspace/schema-cache
      - name: Restore schema cache from generate-sql job
        run: |
          if [ -d /tmp/workspace/schema-cache/bigquery_etl_schemas ]; then
            cp -r /tmp/workspace/schema-cache/bigquery_etl_schemas /tmp/
            echo "Restored schema cache from generate-sql job"
          fi
      - name: Move generated-sql into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/generated-sql/sql sql
      - name: Pull in generated-sql branch from remote
        run: |
          git clone --single-branch --branch generated-sql \
            git@github.com:mozilla/bigquery-etl \
            generated-sql
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_STAGE_SERVICE_ACCOUNT_EMAIL }}
      - name: Deploy changes to stage
        timeout-minutes: 30
        run: |
          if [ "${{ github.event.inputs.skip-stage-deploys }}" = "false" ]; then
            PATHS="$(git diff --no-index --name-only --diff-filter=d generated-sql/sql sql)" || true
            echo $PATHS
            PATH=".venv/bin:$PATH" script/bqetl stage deploy \
              $PATHS
          fi
      - name: Copy generated SQL to temporary stage directory
        run: |
          mkdir -p /tmp/workspace/staged-generated-sql
          cp -r sql/ /tmp/workspace/staged-generated-sql/sql
          cp -r tests/ /tmp/workspace/staged-generated-sql/tests
      - uses: actions/upload-artifact@v4
        with:
          name: staged-generated-sql
          path: /tmp/workspace/staged-generated-sql

  dry-run-sql:
    name: Dry Run SQL
    runs-on: ubuntu-latest
    needs: [deploy-changes-to-stage]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: staged-generated-sql
          path: /tmp/workspace/staged-generated-sql
      - name: Move sql deployed on stage into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/staged-generated-sql/sql sql
          rm -rf tests/
          cp -r /tmp/workspace/staged-generated-sql/tests tests
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: Dry run queries
        run: |
          if [ "${{ github.ref_name }}" = main ]; then
            echo "Skip dryruns on main. Dryruns will run in Airflow"
            exit 0
          elif git log --format=%B --no-merges -n 1 |
              grep -qF '[run-tests]'; then
            echo "Check dry run for all queries because [run-tests] in" \
              "commit message"
            PATHS=sql
          elif [ "${{ github.event.inputs.skip-stage-deploys }}" = "true" ]; then
            PATHS=sql
          else
            PATHS="sql/bigquery-etl-integration-test"
          fi
          echo $PATHS
          PATH=".venv/bin:$PATH" script/bqetl dryrun --validate-schemas $PATHS

  test-sql:
    name: Test SQL
    runs-on: ubuntu-latest
    needs: [deploy-changes-to-stage]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: staged-generated-sql
          path: /tmp/workspace/staged-generated-sql
      - name: Move sql deployed on stage into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/staged-generated-sql/sql sql
          rm -rf tests/
          cp -r /tmp/workspace/staged-generated-sql/tests tests
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: Run SQL tests
        run: |
          PATH=".venv/bin:$PATH" script/entrypoint -m sql -n 8 -p no:bigquery_etl.pytest_plugin.routine

  test-routines:
    name: Test Routines
    runs-on: ubuntu-latest
    needs: [deploy-changes-to-stage, decide-runs]
    if: needs.decide-runs.outputs.validate-routines == 'true' || needs.decide-runs.outputs.deploy == 'true'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: staged-generated-sql
          path: /tmp/workspace/staged-generated-sql
      - name: Move sql deployed on stage into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/staged-generated-sql/sql sql
          rm -rf tests/
          cp -r /tmp/workspace/staged-generated-sql/tests tests
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: Run routine tests
        run: |
          PATH=".venv/bin:$PATH" script/entrypoint -m routine -n 8
      - name: Validate doc examples
        run: |
          PATH=".venv/bin:$PATH" script/bqetl routine validate --docs-only

  validate-views:
    name: Validate Views
    runs-on: ubuntu-latest
    needs: [deploy-changes-to-stage]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: staged-generated-sql
          path: /tmp/workspace/staged-generated-sql
      - name: Move sql deployed on stage into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/staged-generated-sql/sql sql
          rm -rf tests/
          cp -r /tmp/workspace/staged-generated-sql/tests tests
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Validate views
        run: |
          PATH=".venv/bin:$PATH" script/bqetl view validate

  validate-metadata:
    name: Validate Metadata
    needs: [build, decide-runs]
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Validate workgroup access configuration on main
        run: |
          PATH=".venv/bin:$PATH" script/bqetl metadata validate-workgroups sql/
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Export ID Token for Python
        env:
          GOOGLE_GHA_ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: echo "GOOGLE_GHA_ID_TOKEN=$GOOGLE_GHA_ID_TOKEN" >> $GITHUB_ENV
      - name: Verify that metadata files are valid
        run: |
          PATH=".venv/bin:$PATH" script/bqetl query validate \
            --no-dryrun --skip-format-sql
      - name: Copy generated SQL for debugging
        if: failure()
        run: |
          mkdir -p /tmp/debug_artifacts
          cp -r sql /tmp/debug_artifacts/sql
      - name: Upload debug artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: validate-metadata-debug-artifacts
          path: /tmp/debug_artifacts
  validate-backfills:
    name: Validate Backfills
    runs-on: ubuntu-latest
    needs: [generate-sql]
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql
          path: /tmp/workspace/generated-sql
      - name: Move generated-sql into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/generated-sql/sql sql
      - name: Verify that backfill.yaml files are valid
        run: |
          PATH=".venv/bin:$PATH" script/bqetl backfill validate

  private-generate-sql:
    name: Generate Private SQL
    runs-on: ubuntu-latest
    needs: [generate-sql]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql
          path: /tmp/workspace/generated-sql
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_DRYRUN_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Setup SSH key for private repo
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.DEPLOY_KEY_PRIVATE_BQETL }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan github.com >> ~/.ssh/known_hosts
      - name: Pull down private SQL content
        run: |
          git clone --branch main --depth 1 \
            git@github.com:mozilla/private-bigquery-etl.git ~/private-bigquery-etl
      - name: Generate SQL content
        run: |
          export PATH="$PWD/.venv/bin:$PATH"
          export PYTHONPATH="$PWD:$PYTHONPATH"
          bqetl_script="$PWD/script/bqetl"
          cd ~/private-bigquery-etl

          $bqetl_script generate all --target-project moz-fx-data-shared-prod
          $bqetl_script format sql/
          $bqetl_script dependency record --skip-existing sql/
          $bqetl_script metadata update sql/
          $bqetl_script monitoring update sql/

          mkdir -p /tmp/workspace/private-generated-sql
          cp -r /tmp/workspace/generated-sql/sql /tmp/workspace/private-generated-sql/sql
          cp -rf sql/. /tmp/workspace/private-generated-sql/sql
          cp bqetl_project.yaml /tmp/workspace/private-generated-sql/

          git rev-parse HEAD > /tmp/workspace/PRIVATE_BIGQUERY_ETL_SHA
      - uses: actions/upload-artifact@v4
        with:
          name: private-generated-sql
          path: /tmp/workspace/private-generated-sql
      - uses: actions/upload-artifact@v4
        with:
          name: private-bigquery-etl-sha
          path: /tmp/workspace/PRIVATE_BIGQUERY_ETL_SHA

  generate-dags:
    name: Generate DAGs
    runs-on: ubuntu-latest
    needs: [generate-sql, private-generate-sql]
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - uses: actions/download-artifact@v4
        with:
          name: schema-cache
          path: /tmp/workspace/schema-cache
      - uses: actions/download-artifact@v4
        with:
          name: private-generated-sql
          path: /tmp/workspace/private-generated-sql
      - name: Restore schema cache from generate-sql job
        run: |
          if [ -d /tmp/workspace/schema-cache/bigquery_etl_schemas ]; then
            cp -r /tmp/workspace/schema-cache/bigquery_etl_schemas /tmp/
            echo "Restored schema cache from generate-sql job"
          fi
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          token_format: 'id_token'
          id_token_audience: 'https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/dryrun'
          id_token_include_email: true
      - name: Setup SSH key for private repo
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.DEPLOY_KEY_PRIVATE_BQETL }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan github.com >> ~/.ssh/known_hosts
      - name: Pull down private-bigquery-etl content
        run: |
          git clone --branch main --depth 1 \
            git@github.com:mozilla/private-bigquery-etl.git ~/private-bigquery-etl
      - name: Generate DAGs
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/private-generated-sql/sql sql

          cat ~/private-bigquery-etl/dags.yaml | grep -v '^---$' >> dags.yaml

          mkdir -p /tmp/workspace/generated-sql/dags
          PATH=".venv/bin:$PATH" script/bqetl dag generate --output-dir=/tmp/workspace/generated-sql/dags
          mkdir -p /tmp/workspace/private-generated-sql/dags
          mv /tmp/workspace/generated-sql/dags/private_bqetl_*.py /tmp/workspace/private-generated-sql/dags/
      - uses: actions/upload-artifact@v4
        with:
          name: generated-sql-with-dags
          path: /tmp/workspace/generated-sql
      - uses: actions/upload-artifact@v4
        with:
          name: private-generated-sql-with-dags
          path: /tmp/workspace/private-generated-sql

  validate-dags:
    name: Validate DAGs
    runs-on: ubuntu-latest
    needs: [generate-dags]
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Pull telemetry-airflow
        run: |
          git clone https://github.com/mozilla/telemetry-airflow.git ~/telemetry-airflow
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql-with-dags
          path: /tmp/workspace/generated-sql
      - name: Replace telemetry-airflow DAGs with BigQuery ETL DAGs
        run: |
          rm ~/telemetry-airflow/dags/* -f || true
          cp -a /tmp/workspace/generated-sql/dags/. ~/telemetry-airflow/dags/
      - name: Cache telemetry-airflow dependencies
        uses: actions/cache@v4
        with:
          path: ~/telemetry-airflow/.venv
          # yamllint disable-line rule:line-length
          key: telemetry-airflow-deps-v1-${{ hashFiles('~/telemetry-airflow/requirements.txt', '~/telemetry-airflow/requirements-dev.txt', '~/telemetry-airflow/requirements-override.txt') }}
          restore-keys: |
            telemetry-airflow-deps-v1-
      - name: Install telemetry-airflow dependencies
        run: |
          cd ~/telemetry-airflow
          python3 -m venv .venv
          source .venv/bin/activate
          pip install --no-deps -r requirements.txt
          pip install --no-deps -r requirements-dev.txt
          pip install --no-deps -r requirements-override.txt
      - name: ðŸ§ª Test valid DAGs
        run: |
          cd ~/telemetry-airflow
          source .venv/bin/activate
          python -m pytest tests/dags/test_dag_validity.py --junitxml=~/telemetry-airflow/test-results/junit.xml
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: dag-test-results
          path: ~/telemetry-airflow/test-results/junit.xml

  push-generated-sql:
    name: Push Generated SQL
    runs-on: ubuntu-latest
    needs: [validate-dags]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql-with-dags
          path: /tmp/workspace/generated-sql
      - name: Push to generated-sql branch
        run: |
          git config --global user.name "GitHub Actions generate-sql job"
          git config --global user.email "dataops+generated-sql@mozilla.com"
          git clone --single-branch --branch generated-sql \
            git@github.com:mozilla/bigquery-etl \
            generated-sql
          cd generated-sql/
          rm -rf sql/
          cp -r /tmp/workspace/generated-sql/sql sql
          rm -rf dags/
          cp -r /tmp/workspace/generated-sql/dags dags
          git add .
          git commit -m "Auto-push due to change on main branch [ci skip]" \
            && git tag c-${{ github.sha }} \
            && git push \
            && git push origin c-${{ github.sha }} \
            || echo "Skipping push since it looks like there were no changes"

  push-private-generated-sql:
    name: Push Private Generated SQL
    runs-on: ubuntu-latest
    needs: [private-generate-sql, generate-dags]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: private-generated-sql-with-dags
          path: /tmp/workspace/private-generated-sql
      - uses: actions/download-artifact@v4
        with:
          name: private-bigquery-etl-sha
          path: /tmp/workspace/PRIVATE_BIGQUERY_ETL_SHA
      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.DEPLOY_KEY_PRIVATE_BQETL }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan github.com >> ~/.ssh/known_hosts
      - name: Push to private-generated-sql branch
        run: |
          git config --global user.name "GitHub Actions private-generate-sql job"
          git config --global user.email "dataops+private-generated-sql@mozilla.com"
          git clone --single-branch --branch private-generated-sql \
            git@github.com:mozilla/private-bigquery-etl \
            private-generated-sql
          cd private-generated-sql/
          rm -rf sql/
          cp -r /tmp/workspace/private-generated-sql/sql sql
          rm -rf dags/
          cp -r /tmp/workspace/private-generated-sql/dags dags
          git add .

          private_bqetl_sha=$(cat /tmp/workspace/PRIVATE_BIGQUERY_ETL_SHA)

          git commit -m "Auto-push due to change on main branch [ci skip]" \
            && git push \
            || echo "Skipping push since it looks like there were no changes"

  deploy-to-private-gar:
    name: Deploy to Private GAR
    runs-on: ubuntu-latest
    needs: [private-generate-sql, build, generate-sql]
    if: github.ref == 'refs/heads/main'
    permissions:
      id-token: write
      contents: read
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - uses: actions/download-artifact@v4
        with:
          name: generated-sql
          path: /tmp/workspace/generated-sql
      - uses: actions/download-artifact@v4
        with:
          name: private-generated-sql
          path: /tmp/workspace/private-generated-sql
      - name: Move generated-sql into place
        run: |
          rm -rf sql/
          cp -r /tmp/workspace/private-generated-sql/sql sql
          yq -n 'load("/tmp/workspace/private-generated-sql/bqetl_project.yaml") \
            *+ load("/tmp/workspace/generated-sql/bqetl_project.yaml")' > bqetl_project.yaml
      - name: Build the Docker image
        run: |
          docker build . \
            -t us-docker.pkg.dev/moz-fx-data-artifacts-prod/bigquery-etl/bigquery-etl:latest \
            -t us-docker.pkg.dev/moz-fx-data-artifacts-prod/bigquery-etl/bigquery-etl:${{ github.sha }}
      - name: Push Docker image to GAR
        uses: mozilla-it/deploy-actions/docker-push@v4.3.2
        with:
          project_id: moz-fx-data-artifacts-prod
          image_tags: |
            us-docker.pkg.dev/moz-fx-data-artifacts-prod/bigquery-etl/bigquery-etl:latest
            us-docker.pkg.dev/moz-fx-data-artifacts-prod/bigquery-etl/bigquery-etl:${{ github.sha }}
          workload_identity_pool_project_number: ${{ vars.GCPV2_WORKLOAD_IDENTITY_POOL_PROJECT_NUMBER }}
          service_account_name: bigquery-etl

  artifact-deployment:
    name: Trigger Airflow Artifact Deployment
    runs-on: ubuntu-latest
    needs: [deploy-to-private-gar]
    if: github.ref == 'refs/heads/main'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - name: Authenticate to GCP and Generate ID Token
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          token_format: id_token
          # yamllint disable-line rule:line-length
          id_token_audience: https://us-west1-moz-fx-telemetry-airflow-prod.cloudfunctions.net/ci-external-trigger
          id_token_include_email: true
          create_credentials_file: false
      - name: Prepare DAG run note
        # yamllint disable-line rule:line-length
        run: echo "DAGRUN_NOTE=DAG triggered by **[${{ github.actor }}](https://github.com/${{ github.actor }})** from ${{ github.repository }} CI build [${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_ENV
      - name: Trigger bqetl-artifact-deployment in Airflow
        env:
          ID_TOKEN: ${{ steps.auth.outputs.id_token }}
        run: |
          curl --location --request POST "https://us-west1-moz-fx-telemetry-airflow-prod.cloudfunctions.net/ci-external-trigger" \
            -H "Authorization: bearer ${ID_TOKEN}" \
            -H "Content-Type:application/json" \
            -d "{\"dagrun_note\": \"${DAGRUN_NOTE}\", \"dag_id\": \"bqetl_artifact_deployment\"}"

  reset-stage-env:
    name: Reset Stage Environment
    runs-on: ubuntu-latest
    needs: [push-generated-sql, test-sql, validate-views, validate-metadata, dry-run-sql, test-routines]
    if: github.ref == 'refs/heads/main'
    permissions:
      id-token: write
      contents: read
    environment: GH Actions
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          filter: blob:none
      - name: Set up Python
        id: setup_python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      - name: Restore cached virtualenv
        uses: actions/cache/restore@v4
        with:
          key: venv-${{ runner.os }}-${{ steps.setup_python.outputs.python-version }}-${{ hashFiles('requirements.txt') }}
          path: .venv
      - name: Authenticate to GCP (OIDC)
        id: auth
        uses: google-github-actions/auth@v3
        with:
          workload_identity_provider: ${{ vars.GCPV2_GITHUB_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          token_format: id_token
          # yamllint disable-line rule:line-length
          id_token_audience: https://us-west1-moz-fx-telemetry-airflow-prod.cloudfunctions.net/ci-external-trigger
          id_token_include_email: true
          create_credentials_file: false
      - name: Delete stage datasets
        run: |
          PATH=".venv/bin:$PATH" script/bqetl stage clean --dataset-suffix="${{ github.sha }}_${{ github.run_id }}" --delete-expired
