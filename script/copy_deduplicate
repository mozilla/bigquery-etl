#!/usr/bin/env python3

"""
Copy a day's data from live ping tables to stable ping tables,
deduplicating on document_id.

By default, the script will process all tables in datasets named
like *_live, copying data into table of the same name in *_stable
datasets. The script can be configured to exclude a list of tables
or to process only a specific list of tables.
"""

from argparse import ArgumentParser
from datetime import datetime

from google.cloud import bigquery

QUERY_TEMPLATE = """\
WITH
  numbered_duplicates AS (
  SELECT
    *,
    ROW_NUMBER() OVER (PARTITION BY document_id ORDER BY submission_timestamp) AS _n
  FROM
    `{source_table_spec}`
  WHERE
    DATE(submission_timestamp) = @submission_date )
  --
SELECT
  * EXCEPT (_n)
FROM
  numbered_duplicates
WHERE
  _n = 1
"""

parser = ArgumentParser(description=__doc__)
parser.add_argument(
    "--project-id",
    default="moz-fx-data-shar-nonprod-efed",
    help="ID of the project in which to run query jobs",
)
parser.add_argument(
    "--date",
    required=True,
    type=lambda d: datetime.strptime(d, "%Y-%m-%d").date(),
    help="Which day's data to copy, in format 2019-01-01",
)
parser.add_argument(
    "--dry-run",
    action="store_true",
    help=("Do not run queries, but instead print the query job config "
          "and bytes that would be processed"),
)
group = parser.add_mutually_exclusive_group()
group.add_argument(
    "--only",
    nargs="+",
    dest="only_tables",
    default=[],
    help=("Process only the given tables; "
          "pass names like 'telemetry_live.main_v4'"),
)
group.add_argument(
    "--except",
    nargs="+",
    dest="except_tables",
    default=[],
    help=("Process all tables in *_live datasets except for the given tables; "
          "pass names like 'telemetry_live.main_v4'"),
)


def sql_full_table_id(table):
    return table.full_table_id.replace(":", ".")


def run_deduplication_query(
    client, live_table, stable_table, date, dry_run
):

    sql = QUERY_TEMPLATE.format(source_table_spec=sql_full_table_id(live_table))
    destination = f"{sql_full_table_id(stable_table)}${date:%Y%m%d}"
    job_config = bigquery.QueryJobConfig(
        destination=destination,
        query_parameters=[
            bigquery.ScalarQueryParameter("submission_date", "DATE", date)
        ],
        use_legacy_sql=False,
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        dry_run=dry_run,
    )
    query_job = client.query(sql, job_config)
    if dry_run:
        print(
            "Would process {} bytes: {}".format(
                query_job.total_bytes_processed, query_job.to_api_repr()
            )
        )
    else:
        query_job.result()
        print(
            "Processed {} bytes to populate {}".format(
                query_job.total_bytes_processed, destination_table_spec
            )
        )


def main():
    args = parser.parse_args()

    client = bigquery.Client()
    live_datasets = [
        d for d in client.list_datasets(args.project_id) if d.dataset_id.endswith("_live")
    ]

    for live_dataset in live_datasets:
        stable_dataset_id = live_dataset.dataset_id[:-5] + "_stable"
        for live_table in client.list_tables(live_dataset.reference):
            live_table_spec = f"{live_table.dataset_id}.{live_table.table_id}"
            stable_table = client.get_table('.'.join([args.project_id, stable_dataset_id, live_table.table_id]))
            if live_table_spec not in args.only_tables:
                continue
            if live_table_spec in args.except_tables:
                continue
            run_deduplication_query(
                client,
                live_table,
                stable_table,
                args.date,
                args.dry_run,
            )


if __name__ == "__main__":
    main()
