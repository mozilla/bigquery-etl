#!/usr/bin/env python3

"""
Export table data as JSON to a publicly accessible bucket. 

Data of existing BigQuery tables is exported if "json_export" is set in 
the corresponding metadata file.
"""

from argparse import ArgumentParser
import os
import sys
import datetime

from google.cloud import bigquery

# sys.path needs to be modified to enable package imports from parent
# and sibling directories. Also see:
# https://stackoverflow.com/questions/6323860/sibling-package-imports/23542795#23542795
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from bigquery_etl.parse_metadata import Metadata  # noqa E402


DEFAULT_PATTERN = "mozilla-public-data:*.*"
DEFAULT_BUCKET = ""
METADATA_FILE = "metadata.yaml"

parser.add_argument(
    "patterns",
    metavar="[project:]dataset[.table]",
    default=[DEFAULT_PATTERN],
    nargs="*",
    help="Table that should have a latest-version view, may use shell-style wildcards,"
    f" defaults to: {DEFAULT_PATTERN}",
)
parser.add_argument(
    "--target-bucket",
    "--target_bucket",
    default=DEFAULT_BUCKET,
    help="GCP bucket JSON data is exported to.",
)
parser.add_argument(
    "--target-project",
    "--target_project",
    default="moz-fx-data-derived-datasets",
    help="Create views in the target project",
)
parser.add_argument("--target", help="File or directory containing metadata files")
parser.add_argument("--date", help="Submission timestamp [yyyy-mm-dd] of data to be exported")


def get_tables(client, patterns):
    all_projects = None
    all_datasets = {}
    all_tables = {}
    matching_tables = []

    for pattern in patterns:
        project, _, dataset_table = pattern.partition(":")
        dataset, _, table = dataset_table.partition(".")
        projects = [project or client.project]
        dataset = dataset or "*"
        table = table or "*"
        if uses_wildcards(project):
            if all_projects is None:
                all_projects = [p.project_id for p in client.list_projects()]
            projects = [p for p in all_projects if fnmatchcase(project, p)]
        for project in projects:
            datasets = [dataset]
            if uses_wildcards(dataset):
                if project not in all_datasets:
                    all_datasets[project] = [
                        d.dataset_id for d in client.list_datasets(project)
                    ]
                datasets = [d for d in all_datasets[project] if fnmatchcase(d, dataset)]
            for dataset in datasets:
                dataset = f"{project}.{dataset}"
                tables = [(f"{dataset}.{table}", None)]
                if uses_wildcards(table):
                    if dataset not in all_tables:
                        all_tables[dataset] = list(client.list_tables(dataset))
                    tables = [
                        (dataset, t.table_id)
                        for t in all_tables[dataset]
                        if fnmatchcase(t.table_id, table)
                    ]
                    matching_tables += tables

    return matching_tables


def publish_json(client, dataset, table, version, date, bucket):
    # "*" makes sure that files larger than 1GB get split up into multiple JSON files
    destination_uri = "gs://{bucket}/{dataset}/{table}/{verions}/files/{date}/*"

    # todo: explicitly set project?
    dataset_ref = client.dataset(dataset)
    table_ref = dataset_ref.table(table)

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        location="US",  # todo
    )
    extract_job.result()


def main():
    args = parser.parse_args()

    client = bigquery.Client(args.target_project)
    all_tables = get_tables(client, args.patterns)
    date = datetime.strptime(args.date, '%Y-%m-%d')

    if os.path.isdir(args.target):
        for (dataset, table) in get_tables(client, args.patterns):
            metadata_file = os.path.join(args.target, dataset, table, METADATA_FILE)

            try:
                metadata = Metadata.from_file(metadata_file)

                if metadata.is_public_json():
                    # todo: handle incremental and non-incremental differently
                    publish_json(client, dataset, table, date, args.target_bucket)
            except FileNotFoundError:
                print("No metadata file for: {}.{}".format(dataset, table))
    else:
        print(
            """
            Invalid target: {}, target must be a directory with
            structure /<dataset>/<table>/metadata.yaml.
            """.format(
                args.target
            )
        )


if __name__ == "__main__":
    main()
