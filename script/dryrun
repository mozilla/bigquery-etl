#!/usr/bin/env python3

"""
Passes all queries defined under sql/ to a Cloud Function that will run the
queries with the dry_run option enabled.

We could provision BigQuery credentials to the CircleCI job to allow it to run
the queries directly, but there is no way to restrict permissions such that
only dry runs can be performed. In order to reduce risk of CI or local users
accidentally running queries during tests and overwriting production data, we
proxy the queries through the dry run service endpoint.
"""

from multiprocessing.pool import ThreadPool
from os.path import basename, dirname
from urllib.request import urlopen, Request
import glob
import json
import sys


DRY_RUN_URL = (
    "https://us-central1-moz-fx-data-shared-prod.cloudfunctions.net/bigquery-etl-dryrun"
)
DERIVED_DATASETS_DRY_RUN_URL = (
    "https://us-central1-moz-fx-data-derived-datasets.cloudfunctions.net/"
    "bigquery-etl-dryrun"
)

SKIP = {
    # Access Denied
    "sql/activity_stream/tile_id_types/view.sql",
    "sql/monitoring/document_sample_nonprod_v1/query.sql",
    "sql/monitoring/schema_error_counts_v1/view.sql",
    "sql/monitoring/structured_error_counts_v1/view.sql",
    "sql/telemetry/fxa_content_events_v1/query.sql",
    "sql/telemetry/fxa_auth_bounce_events_v1/query.sql",
    "sql/telemetry/fxa_auth_events_v1/query.sql",
    "sql/telemetry/fxa_oauth_events_v1/query.sql",
    "sql/search_derived/search_clients_last_seen_v1/init.sql",
    "sql/search_derived/search_clients_last_seen_v1/query.sql",
    "sql/search/search_rfm/view.sql",
    # Already exists (and lacks an "OR REPLACE" clause)
    "sql/org_mozilla_firefox_derived/clients_first_seen_v1/init.sql",
    "sql/org_mozilla_firefox_derived/clients_last_seen_v1/init.sql",
    "sql/org_mozilla_fenix_derived/clients_last_seen_v1/init.sql",
    "sql/org_mozilla_vrbrowser_derived/clients_last_seen_v1/init.sql",
    "sql/telemetry_derived/core_clients_last_seen_v1/init.sql",
    "sql/telemetry/fxa_users_last_seen_raw_v1/init.sql",
    "sql/telemetry_derived/core_clients_first_seen_v1/init.sql",
    "sql/telemetry_derived/fxa_users_services_last_seen_v1/init.sql",
    # Reference table not found
    "sql/monitoring/structured_detailed_error_counts_v1/view.sql",
    # Access denied for search
    "sql/search_derived/search_aggregates_v8/query.sql",
    "sql/search/search_aggregates/view.sql",
    "sql/search/search_clients_daily/view.sql",
    "sql/search_derived/mobile_search_aggregates_v1/query.sql",
    "sql/search/mobile_search_aggregates/view.sql",
    "sql/search/mobile_search_clients_daily/view.sql",
    # No matching signature for function IF
    "sql/static/fxa_amplitude_export_users_last_seen/query.sql",
    # Duplicate UDF
    "sql/static/fxa_amplitude_export_users_daily/query.sql",
    # Syntax error
    "sql/telemetry_derived/clients_last_seen_v1/init.sql",
    # HTTP Error 408: Request Time-out
    "sql/telemetry_derived/latest_versions/query.sql",
    # Query parameter not found
    "sql/telemetry_derived/experiments_v1/query.sql",
    "sql/telemetry_derived/clients_histogram_aggregates_v1/query.sql",
    "sql/telemetry_derived/clients_histogram_aggregates_old_v1/query.sql",
    "sql/telemetry_derived/clients_histogram_aggregates_merged_v1/query.sql",
    # Dataset moz-fx-data-shared-prod:glam_etl was not found
    "sql/glam_etl/fenix_clients_scalar_aggregates_v1/query.sql",
    "sql/glam_etl/fenix_clients_scalar_aggregates_v1/init.sql",
    "sql/glam_etl/fenix_clients_scalar_bucket_counts_v1/query.sql",
    "sql/glam_etl/fenix_clients_scalar_probe_counts_v1/query.sql",
    "sql/glam_etl/fenix_scalar_percentiles_v1/query.sql",
    "sql/glam_etl/fenix_view_clients_daily_scalar_aggregates_v1/view.sql",
}

USE_DERIVED_DATASETS = {
    "sql/telemetry_derived/ssl_ratios_v1/query.sql",
    "sql/telemetry/fxa_users_last_seen_raw_v1/query.sql",
    "sql/telemetry/firefox_accounts_exact_mau28_raw_v1/query.sql",
    "sql/telemetry/core_clients_daily_v1/query.sql",
    "sql/telemetry/firefox_kpi_dashboard_v1/query.sql",
    "sql/telemetry/core_clients_last_seen_raw_v1/query.sql",
}


def worker_entrypoint(sqlfile):
    sql = open(sqlfile).read()
    if sqlfile in USE_DERIVED_DATASETS:
        url = DERIVED_DATASETS_DRY_RUN_URL
    else:
        url = DRY_RUN_URL
    try:
        r = urlopen(
            Request(
                url,
                headers={"Content-Type": "application/json"},
                data=json.dumps(
                    {"dataset": basename(dirname(dirname(sqlfile))), "query": sql}
                ).encode("utf8"),
                method="POST",
            )
        )
    except Exception as e:
        print(f"{sqlfile:59} ERROR\n", e)
        return False
    response = json.load(r)
    if "errors" in response and len(response["errors"]) == 1:
        error = response["errors"][0]
    else:
        error = None
    if response["valid"]:
        print(f"{sqlfile:59} OK")
    elif (
        error
        and error["code"] == 403
        and "does not have bigquery.tables.create permission for dataset"
        in error["message"]
    ):
        # We want the dryrun service to only have read permissions, so
        # we expect CREATE VIEW and CREATE TABLE to throw specific
        # exceptions.
        print(f"{sqlfile:59} OK")
    else:
        print(f"{sqlfile:59} ERROR\n", response["errors"])
        return False
    return True


def main():
    sql_files = [f for f in glob.glob("sql/**/*.sql", recursive=True) if f not in SKIP]
    with ThreadPool(8) as p:
        result = p.map(worker_entrypoint, sql_files, chunksize=1)
    if all(result):
        exitcode = 0
    else:
        exitcode = 1
    sys.exit(exitcode)


if __name__ == "__main__":
    main()
