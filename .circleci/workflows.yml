---
version: 2.1

orbs:
  gcp-gcr: circleci/gcp-gcr@0.13.0
  docker: circleci/docker@2.2
  python: circleci/python@2.1.1

parameters:
  python-version:
    type: string
    default: '3.11'
  validate-bqetl:
    type: boolean
    default: false
  validate-sql:
    type: boolean
    default: false
  validate-routines:
    type: boolean
    default: false
  deploy:
    type: boolean
    default: false
  trigger-sql-generation:
    type: boolean
    default: false
  skip-stage-deploys:
    type: boolean
    default: false

executors:
  ubuntu-machine-executor:
    machine:
      image: ubuntu-2004:202111-02

jobs:
  build:
    docker: &docker
      - image: python:<< pipeline.parameters.python-version >>
    steps:
      - checkout
      - &restore_venv_cache
        restore_cache:
          keys:
            # when lock files change, use increasingly general
            # patterns to restore cache
            - &python_cache_key
              # yamllint disable-line rule:line-length
              python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-{{ checksum "requirements.in" }}-{{ checksum "requirements.txt" }}
            # yamllint disable-line rule:line-length
            - python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-{{ checksum "requirements.in" }}-
            # yamllint disable-line rule:line-length
            - python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-
            - python-<< pipeline.parameters.python-version >>-packages-v1-main-
      - &build
        run:
          name: Build
          command: |
            python3 -m venv venv/
            venv/bin/pip install --no-deps -r requirements.txt
            venv/bin/pip-sync --pip-args=--no-deps
      - save_cache:
          paths:
            - venv/
          key: *python_cache_key
  verify-format-sql:
    docker: *docker
    steps:
      - when:
          condition: &validate-sql-or-routines
            or:
              - << pipeline.parameters.validate-sql >>
              - << pipeline.parameters.validate-routines >>
              - << pipeline.parameters.deploy >>
          steps:
            - checkout
            - *restore_venv_cache
            - *build
            - run:
                name: Verify that SQL is correctly formatted
                command: |
                  PATH="venv/bin:$PATH" script/bqetl format --check \
                  $(git ls-tree -d HEAD --name-only)
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - &skip
              run:
                name: Skip
                command: echo "Skipped due to path filtering"
  verify-format-yaml:
    docker: *docker
    steps:
      - checkout
      - *restore_venv_cache
      - *build
      - run:
          name: Yamllint Test
          command: PATH="venv/bin:$PATH" yamllint -c .yamllint.yaml .
  verify-requirements:
    docker: *docker
    steps:
      - checkout
      - *restore_venv_cache
      - *build
      - run:
          name: Verify that requirements.txt contains the right dependencies for
            this python version
          command: |
            venv/bin/pip-compile --allow-unsafe --generate-hashes --quiet
            git diff --exit-code -G '^ *[^# ]' -- requirements.txt
  test-bqetl:
    docker: *docker
    steps:
      - when:
          condition: &validate-bqetl
            or:
              - << pipeline.parameters.validate-bqetl >>
              - << pipeline.parameters.deploy >>
          steps:
            - &skip_forked_pr
              run:
                name: Early return if this build is from a forked PR
                command: |
                  if [ -n "$CIRCLE_PR_NUMBER" ]; then
                    echo "Cannot pass creds to forked PRs," \
                      "so marking this step successful"
                    circleci-agent step halt
                  fi
            - checkout
            - *restore_venv_cache
            - *build
            - &authenticate
              run:
                name: Authenticate to GCP
                command: |
                  export GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcp.json"
                  echo 'export GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcp.json"' >> "$BASH_ENV"
                  echo "$GCLOUD_SERVICE_KEY" > "$GOOGLE_APPLICATION_CREDENTIALS"
            - run:
                name: PyTest with linters
                # integration tests are run in a separate `integration` step;
                # SQL and routine tests are split out into a separate `test-sql` test
                # since those tests take the longest to run and running those tests
                # in parallel speeds up CI
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint --black --flake8 \
                    --isort --mypy-ignore-missing-imports --pydocstyle \
                    -m "not (routine or sql or integration)" \
                    -n 8
      - unless:
          condition: *validate-bqetl
          steps:
            - *skip
  test-sql:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - &attach_generated_sql
              attach_workspace:
                at: /tmp/workspace
            - &copy_staged_sql
              run:
                name: Move sql deployed on stage into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/staged-generated-sql/sql sql
                  rm -rf tests/
                  cp -r /tmp/workspace/staged-generated-sql/tests tests
            - run:
                name: Run SQL tests
                command: |
                    PATH="venv/bin:$PATH" script/entrypoint -m sql -n 8
            - &copy_debug_sql
              run:
                name: Copy generated SQL to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r sql /tmp/debug_artifacts/sql
                when: on_fail
            - &copy_debug_tests
              run:
                name: Copy generated tests to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r tests /tmp/debug_artifacts/tests
                when: on_fail
            - &store_debug_artifacts
              store_artifacts:
                path: /tmp/debug_artifacts
                destination: /
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  dry-run-sql:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Dry run queries
                # yamllint disable rule:line-length
                # Dry runs on PRs are executed on sql/bigquery-etl-integration-test
                # Artifacts (queries, views, UDFs) that are changed will be moved to the
                # bigquery-etl-integration-test folder and deployed to the corresponding
                # project. This ensures that dry runs can be executed before changes
                # have been deployed to prod. (bigquery-etl-integration-test is treated
                # as a stage environment)
                command: |
                  if [ "$CIRCLE_BRANCH" = main ]; then
                    echo "Check dry run for all queries because branch is" \
                      "$CIRCLE_BRANCH"
                    PATHS=sql
                  elif git log --format=%B --no-merges -n 1 |
                      grep -qF '[run-tests]'; then
                    echo "Check dry run for all queries because [run-tests] in" \
                      "commit message"
                    PATHS=sql
                  elif [ "<< pipeline.parameters.skip-stage-deploys >>" = "true" ]; then
                    PATHS=sql
                  else
                    PATHS="sql/bigquery-etl-integration-test"
                  fi
                  echo $PATHS
                  PATH="venv/bin:$PATH" script/bqetl dryrun --validate-schemas $PATHS
                # yamllint enable rule:line-length
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  validate-backfills:
    docker: *docker
    steps:
      - when:
          condition: &validate-sql
            or:
              - << pipeline.parameters.validate-sql >>
              - << pipeline.parameters.deploy >>
          steps:
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - run:
                name: Verify that backfill.yaml files are valid
                command: |
                  PATH="venv/bin:$PATH" script/bqetl backfill validate
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  validate-metadata:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Verify that metadata files are valid
                command: |
                  # TODO: Add check here to make sure all queries have metadata.yaml
                  PATH="venv/bin:$PATH" script/bqetl query validate \
                    --respect-dryrun-skip
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  integration:
    docker: *docker
    steps:
      - when:
          condition: *validate-bqetl
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - run:
                name: PyTest Integration Test
                # yamllint disable rule:line-length
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint -m 'integration' -n 8
                # workaround for job failing with `Too long with no output (exceeded 10m0s): context deadline exceeded` error
                no_output_timeout: 30m
      - unless:
          condition: *validate-bqetl
          steps:
            - *skip
  generate-dags:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - &copy_generated_sql
              run:
                name: Move generated-sql into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/generated-sql/sql sql
            - *authenticate
            - run:
                name: Generate DAGs
                command: |
                  mkdir -p /tmp/workspace/generated-sql/dags
                  PATH="venv/bin:$PATH" script/bqetl dag generate --output-dir=/tmp/workspace/generated-sql/dags
            # this task is overwriting the content produced by generate-sql;
            # the behaviour here is additive, generated DAGs are just added to
            # the generated-sql output
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - generated-sql
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  validate-dags:
    executor:
      name: python/default
      tag: 3.11.8
    steps:
      - when:
          condition: *validate-sql
          steps:
            - checkout
            - run:
                name: Pull telemetry-airflow
                command: |
                  git clone https://github.com/mozilla/telemetry-airflow.git ~/telemetry-airflow
            - *attach_generated_sql
            - *copy_generated_sql
            - run:
                name: Replace telemetry-airflow DAGs with BigQuery ETL DAGs
                command: |
                  rm ~/telemetry-airflow/dags/* -f || true
                  cp -a /tmp/workspace/generated-sql/dags/. ~/telemetry-airflow/dags/
            - run:
                name: Install telemetry-airflow dependencies
                command: |
                  cd ~/telemetry-airflow
                  virtualenv .venv
                  source .venv/bin/activate
                  pip install -r requirements.txt
                  pip install -r requirements-dev.txt
            - run:
                name: 🧪 Test valid DAGs
                command: |
                  cd ~/telemetry-airflow
                  source .venv/bin/activate
                  python -m pytest tests/dags/test_dag_validity.py --junitxml=~/telemetry-airflow/test-results/junit.xml
            - store_test_results:
                path: ~/telemetry-airflow/test-results/junit.xml
            - &copy_debug_dags
              run:
                name: Copy generated DAGs to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r dags /tmp/debug_artifacts/dags
                when: on_fail
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  test-routines:
    docker: *docker
    steps:
      - when:
          condition: &validate-routines
            or:
              - << pipeline.parameters.validate-routines >>
              - << pipeline.parameters.deploy >>
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - run:
                name: Run routine tests
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint -m routine -n 8
            - run:
                name: Validate doc examples
                command: |
                  PATH="venv/bin:$PATH" script/bqetl routine validate --docs-only
            - *copy_debug_sql
            - *copy_debug_tests
            - *store_debug_artifacts
      - unless:
          condition: *validate-routines
          steps:
            - *skip
  validate-views:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Validate views
                command: |
                  PATH="venv/bin:$PATH" script/bqetl view validate
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  docs:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Build and deploy docs
                command: |
                  rm -r sql/ && cp -r /tmp/workspace/generated-sql/sql sql/
                  PATH="venv/bin:$PATH" script/bqetl docs generate \
                    --output_dir=generated_docs/
                  cd generated_docs/
                  PATH="../venv/bin:$PATH" mkdocs gh-deploy \
                    -m "[ci skip] Deployed {sha} with MkDocs version: {version}"
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  generate-sql:
    docker: *docker
    resource_class: large
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *authenticate
            - run:
                name: Generate SQL content
                command: |
                  # Check if the generated-sql branch can simply be re-used and SQL generation can be skipped.
                  git clone -b generated-sql git@github.com:mozilla/bigquery-etl ~/remote-generated-sql
                  cd ~/remote-generated-sql

                  # Commits on generated-sql are tagged with the sha1 hash of the commit on main that triggered the SQL generation
                  export LATEST_TAG_GENERATED_SQL=`git describe --tags --abbrev=0`
                  LATEST_TAG_GENERATED_SQL=${LATEST_TAG_GENERATED_SQL/c-/""}
                  echo "Latest tag on generated-sql branch: c-$LATEST_TAG_GENERATED_SQL"

                  # Get the state in which main was in when the generated-sql branch was created and pushed
                  cd ~
                  git clone -b main git@github.com:mozilla/bigquery-etl ~/remote-bigquery-etl-main
                  cd ~/remote-bigquery-etl-main
                  export LATEST_COMMIT_ON_MAIN=`git rev-parse --short HEAD`
                  git checkout $LATEST_TAG_GENERATED_SQL
                  echo "Latest commit on main: $LATEST_COMMIT_ON_MAIN"

                  cd ~/project
                  export GENERATED_SQL_CHANGED=false

                  # Manually trigger SQL generation via a pipeline parameter
                  if [ "<< pipeline.parameters.trigger-sql-generation >>" = "true" ]; then
                    echo "trigger-sql-generation was set to true; run SQL generation"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If this task runs when merging into main, then always run the SQL generation
                  if [ "$CIRCLE_BRANCH" = main ]; then
                    echo "On main branch; generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # Check if this PR removes a file. SQL generation needs to be re-run because
                  # simply merging the changes with the generated-sql branch would result in files being part of generated DAGs
                  # and other artifacts that were actually removed
                  if [[ $(git diff --no-index --name-only --diff-filter=D ~/remote-bigquery-etl-main/sql sql) ]]; then
                    echo "SQL file was removed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # Check if this PR changes SQL generators
                  if [[ $(diff -qr --no-dereference sql_generators ~/remote-bigquery-etl-main/sql_generators) ]]; then
                    echo "SQL generators changed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  mkdir -p /tmp/workspace/generated-sql

                  if [ "$GENERATED_SQL_CHANGED" = "true" ]; then
                    echo "Changes made will affect generated SQL. Run SQL generators."

                    cp -r sql/ /tmp/workspace/generated-sql/sql
                    # Don't depend on dry run for PRs
                    PATH="venv/bin:$PATH" script/bqetl generate all \
                      --output-dir /tmp/workspace/generated-sql/sql/ \
                      --target-project moz-fx-data-shared-prod
                  else
                    echo "Changes made don't affect generated SQL. Use content from generated-sql"

                    # merge files that changed in this PR with the generated-sql branch
                    cp -r ~/remote-generated-sql/sql/ /tmp/workspace/generated-sql/sql
                    cp -a sql/. /tmp/workspace/generated-sql/sql
                  fi

                  PATH="venv/bin:$PATH" script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/generated-sql/sql/"
                  PATH="venv/bin:$PATH" script/bqetl metadata update \
                    --sql-dir /tmp/workspace/generated-sql/sql/ \
                    /tmp/workspace/generated-sql/sql/
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - generated-sql
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  deploy-changes-to-stage:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Pull in generated-sql branch from remote
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git clone --single-branch --branch generated-sql \
                    git@github.com:mozilla/bigquery-etl \
                    generated-sql
            - *authenticate
            - run:
                name: Deploy changes to stage
                command: |
                  if [ "<< pipeline.parameters.skip-stage-deploys >>" = "false" ]; then
                    PATHS="$(git diff --no-index --name-only --diff-filter=d generated-sql/sql sql)" || true
                    echo $PATHS
                    PATH="venv/bin:$PATH" script/bqetl stage deploy \
                      --dataset-suffix=$CIRCLE_SHA1 \
                      --remove-updated-artifacts \
                      $PATHS
                  fi
                # workaround for job failing with `Too long with no output (exceeded 10m0s): context deadline exceeded` error
                no_output_timeout: 30m
            - run:
                name: Copy generated SQL to temporary stage directory
                command: |
                  mkdir -p /tmp/workspace/staged-generated-sql
                  cp -r sql/ /tmp/workspace/staged-generated-sql/sql
                  cp -r tests/ /tmp/workspace/staged-generated-sql/tests
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - staged-generated-sql
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  push-generated-sql:
    docker: *docker
    steps:
      - when:
          condition: &deploy
            or:
              - << pipeline.parameters.deploy >>
          steps:
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Push to generated-sql branch
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.name "CircleCI generate-sql job"
                  git config --global user.email "dataops+generated-sql@mozilla.com"
                  git clone --single-branch --branch generated-sql \
                    git@github.com:mozilla/bigquery-etl \
                    generated-sql
                  cd generated-sql/
                  rm -rf sql/
                  cp -r /tmp/workspace/generated-sql/sql sql
                  rm -rf dags/
                  cp -r /tmp/workspace/generated-sql/dags dags
                  git add .
                  git commit -m "Auto-push due to change on main branch [ci skip]" \
                    && git tag c-${CIRCLE_SHA1:0:9} \
                    && git push \
                    && git push origin c-${CIRCLE_SHA1:0:9} \
                    || echo "Skipping push since it looks like there were no changes"
      - unless:
          condition: *deploy
          steps:
            - *skip
  sync-dags-repo:
    executor: docker/machine
    parameters:
      repo-to-sync:
        type: string
      target-branch:
        type: string
    steps:
      - when:
          condition: *deploy
          steps:
            - checkout
            - add_ssh_keys:
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
                  - "e4:30:50:41:53:f0:d6:3a:bb:c9:38:54:2d:ca:56:41"
            - run:
                name: 🤖 Update DAGs repository
                # ~/.ssh/config needs to be manually configured to support cloning a private
                # repository through submodules and writing to another repository.
                command: |
                  cat \<<'EOT' > ~/.ssh/config
                  Host github.com
                    IdentitiesOnly yes
                    IdentityFile /home/circleci/.ssh/id_rsa_9d1eaf52782ce8ec334cdbcd5aff700a
                  Host git-telemetry-airflow-dags
                    HostName github.com
                    User git
                    IdentitiesOnly yes
                    IdentityFile /home/circleci/.ssh/id_rsa_e430504153f0d63abbc938542dca5641
                  EOT
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.email "dataops@mozilla.com"
                  git config --global user.name "CircleCI Job"
                  git clone --branch main git@git-telemetry-airflow-dags:mozilla/telemetry-airflow-dags.git
                  cd ~/project/telemetry-airflow-dags
                  git submodule update --init --recursive --depth 1
                  cd << parameters.repo-to-sync >>
                  git pull origin << parameters.target-branch >>
                  cd ..
                  git add << parameters.repo-to-sync >>
                  git commit --allow-empty -m "Automatic commit from ${CIRCLE_PROJECT_REPONAME} commit ${CIRCLE_SHA1:0:9} build ${CIRCLE_BUILD_NUM} [skip ci]"
                  git push origin main
      - unless:
          condition: *deploy
          steps:
            - *skip
  deploy:
    executor: ubuntu-machine-executor
    steps:
      - when:
          condition: *deploy
          steps:
            - checkout
            - *attach_generated_sql
            - *copy_generated_sql
            - docker/check:
                docker-password: DOCKER_PASS
                docker-username: DOCKER_USER
            - docker/build: &public-image
                image: ${CIRCLE_PROJECT_USERNAME+$CIRCLE_PROJECT_USERNAME/}${CIRCLE_PROJECT_REPONAME:-bigquery-etl}
                tag: ${CIRCLE_TAG:-latest}
            - docker/push: *public-image
      - unless:
          condition: *deploy
          steps:
            - *skip
  artifact-deployment:
    docker:
      - image: cimg/base:2024.01
    steps:
      - run:
          name: Trigger bqetl-artifact-deployment in Airflow
          command: >
            curl --location "https://workflow.telemetry.mozilla.org/api/v1/dags/bqetl_artifact_deployment/dagRuns"
            --header "Content-Type: application/json"
            --header "Accept: application/json"
            --header "Authorization: Basic ${AIRFLOW_BQETL_CI_USER_BASIC_AUTH}"
            --data "{
              \"note\": \"DAG Triggered by **[${CIRCLE_USERNAME}](https://github.com/${CIRCLE_USERNAME})** from ${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME} CI build [${CIRCLE_BUILD_NUM}](${CIRCLE_BUILD_URL}).\"
            }"
  private-generate-sql:
    docker: *docker
    steps:
      - when:
          condition: *deploy
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *authenticate
            - add_ssh_keys:
                # deploy key to private-bigquery-etl
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
            - run:
                name: Install rsync
                command: |
                  apt update
                  apt install -y rsync
            - run:
                name: Pull down private SQL content
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git clone --single-branch --branch main \
                                git@github.com:mozilla/private-bigquery-etl.git \
                                ~/private-bigquery-etl
                  rsync --archive ~/private-bigquery-etl/sql/ sql/
                  rsync --archive ~/private-bigquery-etl/dags.yaml dags.yaml
            - run:
                name: Generate SQL content
                command: |
                  mkdir -p /tmp/workspace/private-generated-sql

                  cp -r sql/ /tmp/workspace/private-generated-sql/sql
                  # Don't depend on dry run for PRs
                  PATH="venv/bin:$PATH" script/bqetl generate all \
                    --output-dir /tmp/workspace/private-generated-sql/sql/ \
                    --target-project moz-fx-data-shared-prod
                  PATH="venv/bin:$PATH" script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/private-generated-sql/sql/"
                  PATH="venv/bin:$PATH" script/bqetl metadata update \
                    --sql-dir /tmp/workspace/private-generated-sql/sql/ \
                    /tmp/workspace/private-generated-sql/sql/

                  # Change directory to generate DAGs so `sql_file_path` values are relative to the repo root.
                  export PATH="$PWD/venv/bin:$PATH"
                  export PYTHONPATH="$PWD:$PYTHONPATH"
                  bqetl_script="$PWD/script/bqetl"
                  cd ~/private-bigquery-etl
                  mkdir -p /tmp/workspace/private-generated-sql/dags
                  $bqetl_script dag generate \
                    --output-dir /tmp/workspace/private-generated-sql/dags/
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - private-generated-sql
      - unless:
          condition: *deploy
          steps:
            - *skip
  push-private-generated-sql:
    docker: *docker
    steps:
      - when:
          condition: *deploy
          steps:
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
            - run:
                name: Push to private-generated-sql branch
                # yamllint disable rule:line-length
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.name "CircleCI private-generate-sql job"
                  git config --global user.email "dataops+private-generated-sql@mozilla.com"
                  git clone --single-branch --branch private-generated-sql \
                    git@github.com:mozilla/private-bigquery-etl \
                    private-generated-sql
                  cd private-generated-sql/
                  rm -rf sql/
                  cp -r /tmp/workspace/private-generated-sql/sql sql
                  rm -rf dags/
                  cp -r /tmp/workspace/private-generated-sql/dags dags
                  git add .
                  git commit -m "Auto-push due to change on main branch [ci skip]" \
                    && git push \
                    || echo "Skipping push since it looks like there were no changes"
                # yamllint enable rule:line-length
      - unless:
          condition: *deploy
          steps:
            - *skip
  deploy-to-private-gcr:
    executor: ubuntu-machine-executor
    steps:
      - when:
          condition: *deploy
          steps:
            - checkout
            - *attach_generated_sql
            - run:
                name: Move generated-sql into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/private-generated-sql/sql sql
            - gcp-gcr/gcr-auth
            - gcp-gcr/build-image: &private-image
                image: bigquery-etl
                tag: ${CIRCLE_TAG:-latest}
            - gcp-gcr/push-image: *private-image
      - unless:
          condition: *deploy
          steps:
            - *skip
  main-generate-sql-and-dags:
    docker: *docker
    resource_class: large
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - run:
                name: Switch to main branch
                command: |
                  git remote add mozilla git@github.com:mozilla/bigquery-etl
                  git fetch mozilla main
                  git checkout mozilla/main
            - attach_workspace:
                at: /tmp/workspace
            - *restore_venv_cache
            - *build
            - *authenticate
            - run:
                name: Generate SQL content
                command: |
                  export PATH="venv/bin:$PATH"

                  # Check if the generated-sql branch can simply be re-used and SQL generation can be skipped.
                  # There is a delay between pushing changes to main and pushing a new generated-sql branch,
                  # so we need to account for possible changes that happen during this time
                  git clone -b generated-sql git@github.com:mozilla/bigquery-etl ~/remote-generated-sql
                  cd ~/remote-generated-sql

                  # Commits on generated-sql are tagged with the sha1 hash of the commit on main that triggered the SQL generation
                  export LATEST_TAG_GENERATED_SQL=`git describe --tags --abbrev=0`
                  LATEST_TAG_GENERATED_SQL=${LATEST_TAG_GENERATED_SQL/c-/""}

                  cd ~/project
                  export GENERATED_SQL_CHANGED=false

                  # Manually trigger SQL generation via a pipeline parameter
                  if [ "<< pipeline.parameters.trigger-sql-generation >>" = "true" ]; then
                    echo "trigger-sql-generation was set to true; run SQL generation"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If the between main the generated-sql branch SQL generators have changed, then re-run the SQL generation
                  if [[ $(git diff --name-only --diff-filter=d $LATEST_TAG_GENERATED_SQL...HEAD -- sql_generators) ]]; then
                    echo "SQL generator changed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If a SQL was removed from main re-run SQL generation
                  # simply merging the changes with the generated-sql branch would result in files being part of generated DAGs
                  # and other artifacts that were removed
                  if [[ $(git diff --name-only --diff-filter=D $LATEST_TAG_GENERATED_SQL...HEAD -- sql) ]]; then
                    echo "SQL file was removed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  mkdir -p /tmp/workspace/main-generated-sql
                  mkdir -p /tmp/workspace/main-generated-sql/dags

                  if [ "$GENERATED_SQL_CHANGED" = "true" ]; then
                    # re-run SQL generation and re-generate DAGs
                    ./script/bqetl generate all \
                      --target-project moz-fx-data-shared-prod

                    cp -r sql/ /tmp/workspace/main-generated-sql/sql
                  else
                    echo "Changes made don't affect generated SQL. Use content from generated-sql"

                    # merge files that changed in this PR with the generated-sql branch
                    cp -r ~/remote-generated-sql/sql/. /tmp/workspace/main-generated-sql/sql
                    cp -a sql/. /tmp/workspace/main-generated-sql/sql
                  fi

                  ./script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/main-generated-sql/sql/"
                  ./script/bqetl metadata update \
                    --sql-dir /tmp/workspace/main-generated-sql/sql/ \
                    /tmp/workspace/main-generated-sql/sql/
                  # re-generate DAGs

                  ./script/bqetl dag generate \
                    --sql-dir /tmp/workspace/main-generated-sql/sql/ \
                    --output-dir /tmp/workspace/main-generated-sql/dags

                  find /tmp/workspace/main-generated-sql/dags -name '*.py' \
                    -exec sed -i -e 's/\/tmp\/workspace\/main-generated-sql\/sql/sql/g' {} \;
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - main-generated-sql
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  generate-diff:
    docker:
      - image: cimg/node:21.2.0
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - checkout
            - attach_workspace:
                at: /tmp/workspace
            - run:
                name: Generate diff
                command: |
                  diff -qr --no-dereference \
                    /tmp/workspace/main-generated-sql/dags/ /tmp/workspace/generated-sql/dags/ \
                    | grep '^Only in' \
                    > /tmp/workspace/sql.diff || true
                  diff -bur --no-dereference --new-file \
                    /tmp/workspace/main-generated-sql/dags/ /tmp/workspace/generated-sql/dags/ \
                    >> /tmp/workspace/sql.diff || true
                  diff -qr --no-dereference \
                    /tmp/workspace/main-generated-sql/sql/ /tmp/workspace/generated-sql/sql/ \
                    | grep '^Only in' \
                    >> /tmp/workspace/sql.diff || true
                  diff -bur --no-dereference --new-file \
                    /tmp/workspace/main-generated-sql/sql/ /tmp/workspace/generated-sql/sql/ \
                    >> /tmp/workspace/sql.diff || true
            - store_artifacts:
                path: /tmp/workspace/sql.diff
                destination: sql.diff
            - run: npm i circle-github-bot@2.1.0 @octokit/graphql@7.0.2
            - run: .circleci/post-diff.js
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  reset-stage-env:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *build
            - *authenticate
            - run:
                name: "Delete stage datasets"
                command: |
                  PATH="venv/bin:$PATH" script/bqetl stage clean --dataset-suffix=$CIRCLE_SHA1 --delete-expired
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  manual-trigger-required-for-fork:
    docker: *docker
    steps:
      - &skip_upstream
        run:
          name: Early return if this build is running on upstream
          command: |
            if [ -n "$CIRCLE_PR_NUMBER" ]; then
              echo "Build on fork"
            else
              echo "Build on upstream"
              circleci-agent step halt
            fi
      - checkout
      - run:
          name: Manually trigger integration tests for fork
          # yamllint disable rule:line-length
          command: |
            apt update
            apt install jq -y

            CIRCLE_PR_BRANCH=`curl -s https://api.github.com/repos/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/pulls/${CIRCLE_PR_NUMBER} | jq -r '.head.label'`

            echo "Integration tests for this fork need to be triggered manually"
            echo "Users with write access to the repository can trigger" \
              "integration tests by following these steps: "
            echo "  Open the following page:"
            echo "    https://github.com/mozilla/bigquery-etl/actions/workflows/push-to-upstream.yml"
            echo "  Choose the 'Run workflow' dropdown and provide '$CIRCLE_PR_BRANCH' as parameter."

            exit 1
          # yamllint enable rule:line-length
  deploy-to-pypi:
    docker: *docker
    steps:
      - checkout
      - run:
          name: Check for package version change in last commit before proceeding.
          command: |
            if git diff main HEAD~1 pyproject.toml | grep 'version'
              then
                echo "Found changes to package version dir, proceeding with deployment."
              else
                echo "No changes in package version. Skipping bigquery-etl deployment."
                circleci-agent step halt
            fi
      - run:
          name: Install deployment tools
          command: |
            pip install --upgrade setuptools wheel twine
      - run:
          name: Create the distribution files
          command: |
            python -m pip install build
            python -m build --sdist --wheel
      - run:
          name: Upload to PyPI
          command: |
            # Relies on the TWINE_USERNAME and TWINE_PASSWORD environment variables
            # For more on twine, see:
            #   https://twine.readthedocs.io/en/latest/
            twine upload --skip-existing dist/*

workflows:
  version: 2
  build:
    jobs: &build_jobs
      - manual-trigger-required-for-fork
      - build
      - verify-requirements
      - verify-format-yaml
      - verify-format-sql
      - deploy-changes-to-stage:
          requires:
            - generate-sql
      - dry-run-sql:
          requires:
            - deploy-changes-to-stage
      - test-sql:
          requires:
            - deploy-changes-to-stage
      - validate-metadata:
          requires:
            - deploy-changes-to-stage
      - validate-backfills:
          requires:
            - deploy-changes-to-stage
      - validate-dags:
          requires:
            - generate-dags
      - validate-views:
          requires:
            - deploy-changes-to-stage
      - generate-sql
      - main-generate-sql-and-dags:
          filters:
            branches:
              ignore: main
      - generate-diff:
          requires:
            - generate-dags
            - main-generate-sql-and-dags
          filters:
            branches:
              ignore: main
      - generate-dags:
          requires:
            - generate-sql
      - docs:
          requires:
            - generate-sql
          filters:
            branches:
              only: main
      - push-generated-sql:
          requires:
            - validate-dags
          filters:
            branches:
              only:
                - main
      - reset-stage-env:
          requires:
            - push-generated-sql
            - test-sql
            - validate-views
            - validate-metadata
            - dry-run-sql
      - test-routines:
          requires:
            - deploy-changes-to-stage
      - test-bqetl
      - integration
      # The following "private" jobs are basically clones of the public jobs
      # for generate-sql, deploy, and push-generated-sql, except that they pull
      # in some additional content from an internal Mozilla repository for
      # cases where ETL code cannot be public. Although the CI logic is
      # consolidated in this public repository, note that we are both pulling
      # from the internal repository and pushing generated results back to
      # a branch on that internal repository, which may be initially
      # surprising.
      - private-generate-sql:
          filters:
            branches:
              only:
                - main
      - push-private-generated-sql:
          requires:
            - private-generate-sql
          filters:
            branches:
              only:
                - main
      - deploy:
          context: data-eng-bigquery-etl-dockerhub
          requires:
            - generate-sql
            # Public image must be pushed after the private one because of
            # webhooks used in Ops logic. For details, see:
            # https://bugzilla.mozilla.org/show_bug.cgi?id=1715628#c0
            - deploy-to-private-gcr
          filters:
            branches:
              only:
                - main
      - artifact-deployment:
          name: Trigger bqetl_artifact_deployment Airflow DAG
          context:
            - data-eng-circleci-deployment
          requires:
            - deploy
          filters:
            branches:
              only:
                - main
      - sync-dags-repo:
          name: 🔃 Synchronize bigquery-etl submodule
          repo-to-sync: ${CIRCLE_PROJECT_REPONAME}
          target-branch: generated-sql
          requires:
            - push-generated-sql
          filters:
            branches:
              only:
                - main
      - sync-dags-repo:
          name: 🔃 Synchronize private-bigquery-etl submodule
          repo-to-sync: private-bigquery-etl
          target-branch: private-generated-sql
          requires:
            - push-private-generated-sql
            - 🔃 Synchronize bigquery-etl submodule
          filters:
            branches:
              only:
                - main
      - deploy-to-private-gcr:
          context: data-eng-airflow-gcr
          requires:
            - private-generate-sql
            # can't run in parallel because CIRCLE_BUILD_NUM is same
            - build
            - generate-sql
          filters:
            branches:
              only:
                - main
  tagged-deploy:
    jobs:
      - deploy-to-pypi:
          filters:
            branches:
              only:
                - main
