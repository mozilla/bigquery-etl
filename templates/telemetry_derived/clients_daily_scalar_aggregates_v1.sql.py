#!/usr/bin/env python3
"""clients_daily_scalar_aggregates query generator."""
import sys
import json
import argparse
import textwrap
import subprocess
import urllib.request


PROBE_INFO_SERVICE = (
    "https://probeinfo.telemetry.mozilla.org/firefox/all/main/all_probes"
)

p = argparse.ArgumentParser()
p.add_argument(
    "--agg-type",
    type=str,
    help="One of scalar/keyed-scalar/keyed-boolean",
    required=True,
)


def generate_sql(
    agg_type,
    aggregates,
    additional_queries,
    additional_partitions,
    select_clause,
    querying_table,
):
    """Create a SQL query for the clients_daily_scalar_aggregates dataset."""
    return textwrap.dedent(
        f"""-- Query generated by:
            -- templates/clients_daily_scalar_aggregates.sql.py --agg-type {agg_type}
        WITH filtered AS (
            SELECT
                *,
                SPLIT(application.version, '.')[OFFSET(0)] AS app_version,
                DATE(submission_timestamp) as submission_date,
                normalized_os as os,
                application.build_id AS app_build_id,
                normalized_channel AS channel
            FROM `moz-fx-data-shared-prod.telemetry_stable.main_v4`
            WHERE DATE(submission_timestamp) = @submission_date
                AND normalized_channel in (
                  "release", "beta", "nightly"
                )
                AND client_id IS NOT NULL),

        {additional_queries}

        -- Using `min` for when `agg_type` is `count` returns null when all rows are null
        aggregated AS (
            SELECT
                submission_date,
                client_id,
                os,
                app_version,
                app_build_id,
                channel,
                {aggregates}
            FROM {querying_table}
            GROUP BY
                submission_date,
                client_id,
                os,
                app_version,
                app_build_id,
                channel
                {additional_partitions})

            {select_clause}
        """
    )


def get_histogram_type(keyval_fields):
    """Return the type of histogram given its schema."""
    if (
        len(keyval_fields) == 2
        and keyval_fields[0].get("type", None) == "INTEGER"
        and keyval_fields[1].get("type", None) == "INTEGER"
    ):
        return "histogram"

    if (
        len(keyval_fields) == 2
        and keyval_fields[0].get("type", None) == "STRING"
        and keyval_fields[1].get("type", None) == "INTEGER"
    ):
        return "string-histogram"

    if (
        len(keyval_fields) == 2
        and keyval_fields[0].get("type", None) == "STRING"
        and keyval_fields[1].get("type", None) == "RECORD"
        and keyval_fields[1]["fields"][0]["type"] == "INTEGER"
    ):
        return "keyed-histogram"


def _get_generic_keyed_scalar_sql(probes, value_type):
    probes_struct = []
    for probe in probes:
        probes_struct.append(
            f"('{probe}', payload.processes.parent.keyed_scalars.{probe})"
        )

    probes_arr = ",\n\t\t\t".join(probes_struct)

    additional_queries = f"""
        grouped_metrics AS
          (SELECT
            client_id,
            submission_date,
            os,
            app_version,
            app_build_id,
            channel,
            ARRAY<STRUCT<
                name STRING,
                value ARRAY<STRUCT<key STRING, value {value_type}>>
            >>[
              {probes_arr}
            ] as metrics
          FROM filtered),

          flattened_metrics AS
            (SELECT
              client_id,
              submission_date,
              os,
              app_version,
              app_build_id,
              channel,
              metrics.name AS metric,
              value.key AS key,
              value.value AS value
            FROM grouped_metrics
            CROSS JOIN UNNEST(metrics) AS metrics,
            UNNEST(metrics.value) AS value),
    """

    querying_table = "flattened_metrics"

    additional_partitions = """,
                            metric,
                            key
    """

    return {
        "additional_queries": additional_queries,
        "additional_partitions": additional_partitions,
        "querying_table": querying_table,
    }


def get_keyed_boolean_probes_sql_string(probes):
    """Put together the subsets of SQL required to query keyed booleans."""
    sql_strings = _get_generic_keyed_scalar_sql(probes, "BOOLEAN")
    sql_strings[
        "probes_string"
    ] = """
        metric,
        key,
        SUM(CASE WHEN value = True THEN 1 ELSE 0 END) AS true_col,
        SUM(CASE WHEN value = False THEN 1 ELSE 0 END) AS false_col
    """

    sql_strings[
        "select_clause"
    ] = """
        SELECT
              client_id,
              submission_date,
              os,
              app_version,
              app_build_id,
              channel,
              ARRAY_CONCAT_AGG(ARRAY<STRUCT<
                    metric STRING,
                    metric_type STRING,
                    key STRING,
                    agg_type STRING,
                    value FLOAT64
                >>
                [
                    (metric, 'keyed-scalar-boolean', key, 'true', true_col),
                    (metric, 'keyed-scalar-boolean', key, 'false', false_col)
                ]
            ) AS scalar_aggregates
        FROM aggregated
        GROUP BY
            client_id,
            submission_date,
            os,
            app_version,
            app_build_id,
            channel
    """
    return sql_strings


def get_keyed_scalar_probes_sql_string(probes):
    """Put together the subsets of SQL required to query keyed scalars."""
    sql_strings = _get_generic_keyed_scalar_sql(probes, "INT64")
    sql_strings[
        "probes_string"
    ] = """
        metric,
        key,
        MAX(value) AS max,
        MIN(value) AS min,
        AVG(value) AS avg,
        SUM(value) AS sum,
        IF(MIN(value) IS NULL, NULL, COUNT(*)) AS count
    """

    sql_strings[
        "select_clause"
    ] = """
        SELECT
            client_id,
            submission_date,
            os,
            app_version,
            app_build_id,
            channel,
            ARRAY_CONCAT_AGG(ARRAY<STRUCT<
                metric STRING,
                metric_type STRING,
                key STRING,
                agg_type STRING,
                value FLOAT64
            >>
                [
                    (metric, 'keyed-scalar', key, 'max', max),
                    (metric, 'keyed-scalar', key, 'min', min),
                    (metric, 'keyed-scalar', key, 'avg', avg),
                    (metric, 'keyed-scalar', key, 'sum', sum),
                    (metric, 'keyed-scalar', key, 'count', count)
                ]
        ) AS scalar_aggregates
        FROM aggregated
        GROUP BY
            client_id,
            submission_date,
            os,
            app_version,
            app_build_id,
            channel
    """
    return sql_strings


def get_scalar_probes_sql_strings(probes, scalar_type):
    """Put together the subsets of SQL required to query scalars or booleans."""
    if scalar_type == "keyed_scalars":
        return get_keyed_scalar_probes_sql_string(probes["keyed"])

    if scalar_type == "keyed_booleans":
        return get_keyed_boolean_probes_sql_string(probes["keyed_boolean"])

    probe_structs = []
    for probe in probes["scalars"]:
        probe_structs.append((
            f"('{probe}', 'scalar', '', 'max', "
            f"max(CAST(payload.processes.parent.scalars.{probe} AS INT64)))")
        )
        probe_structs.append((
            f"('{probe}', 'scalar', '', 'avg', "
            f"avg(CAST(payload.processes.parent.scalars.{probe} AS INT64)))")
        )
        probe_structs.append((
            f"('{probe}', 'scalar', '', 'min', "
            f"min(CAST(payload.processes.parent.scalars.{probe} AS INT64)))")
        )
        probe_structs.append((
            f"('{probe}', 'scalar', '', 'sum', "
            f"sum(CAST(payload.processes.parent.scalars.{probe} AS INT64)))")
        )
        probe_structs.append(f"('{probe}', 'scalar', '', 'count', IF(MIN(payload.processes.parent.scalars.{probe}) IS NULL, NULL, COUNT(*)))")

    for probe in probes["booleans"]:
        probe_structs.append(
            (
                f"('{probe}', 'boolean', '', 'false', "
                f"SUM(case when payload.processes.parent.scalars.{probe} = False "
                "THEN 1 ELSE 0 END))"
            )
        )
        probe_structs.append(
            (
                f"('{probe}', 'boolean', '', 'true', "
                f"SUM(case when payload.processes.parent.scalars.{probe} = True "
                "THEN 1 ELSE 0 END))"
            )
        )

    probes_arr = ",\n\t\t\t".join(probe_structs)
    probes_string = f"""
            ARRAY<STRUCT<
                metric STRING,
                metric_type STRING,
                key STRING,
                agg_type STRING,
                value FLOAT64
            >> [
                {probes_arr}
            ] AS scalar_aggregates
    """

    select_clause = f"""
        SELECT *
        FROM aggregated
    """

    return {"probes_string": probes_string, "select_clause": select_clause}


def get_scalar_probes(scalar_type):
    """Find all scalar probes in main summary.

    Note: that non-integer scalar probes are not included.
    """
    project = "moz-fx-data-shared-prod"
    main_summary_scalars = set()
    main_summary_record_scalars = set()
    main_summary_boolean_record_scalars = set()
    main_summary_boolean_scalars = set()

    process = subprocess.Popen(
        [
            "bq",
            "show",
            "--schema",
            "--format=json",
            f"{project}:telemetry_stable.main_v4",
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    stdout, stderr = process.communicate()
    if process.returncode > 0:
        raise Exception(
            f"Call to bq exited non-zero: {process.returncode}", stdout, stderr
        )
    main_summary_schema = json.loads(stdout)

    scalars_field = None
    for field in main_summary_schema:
        if field["name"] != "payload":
            continue

        for payload_field in field["fields"]:
            if payload_field["name"] == "processes":
                for processes_field in payload_field["fields"]:
                    if processes_field["name"] == "parent":
                        for parent_field in processes_field["fields"]:
                            if parent_field["name"] == scalar_type:
                                scalars_field = parent_field
                                break

    if scalars_field is None:
        return

    for scalar in scalars_field.get("fields", {}):
        if "name" not in scalar:
            continue

        if scalar.get("type", "") == "INTEGER":
            main_summary_scalars.add(scalar["name"])
        elif scalar.get("type", "") == "BOOLEAN":
            main_summary_boolean_scalars.add(scalar["name"])
        elif scalar.get("type", "") == "RECORD":
            if scalar["fields"][1]["type"] == "BOOLEAN":
                main_summary_boolean_record_scalars.add(scalar["name"])
            else:
                main_summary_record_scalars.add(scalar["name"])

    # Find the intersection between relevant scalar probes
    # and those that exist in main summary
    with urllib.request.urlopen(PROBE_INFO_SERVICE) as url:
        data = json.loads(url.read().decode())
        scalar_probes = set(
            [
                x.replace("scalar/", "").replace(".", "_")
                for x in data.keys()
                if x.startswith("scalar/")
            ]
        )
        return {
            "scalars": scalar_probes.intersection(main_summary_scalars),
            "booleans": scalar_probes.intersection(main_summary_boolean_scalars),
            "keyed": scalar_probes.intersection(main_summary_record_scalars),
            "keyed_boolean": scalar_probes.intersection(
                main_summary_boolean_record_scalars
            ),
        }


def main(argv, out=print):
    """Print a clients_daily_scalar_aggregates query to stdout."""
    opts = vars(p.parse_args(argv[1:]))
    sql_string = ""

    if opts["agg_type"] in ("scalars", "keyed_scalars", "keyed_booleans"):
        scalar_type = (
            opts["agg_type"] if (opts["agg_type"] == "scalars") else "keyed_scalars"
        )
        scalar_probes = get_scalar_probes(scalar_type)
        sql_string = get_scalar_probes_sql_strings(scalar_probes, opts["agg_type"])
    else:
        raise ValueError(
            "agg-type must be one of scalars, keyed_scalars, keyed_booleans"
        )

    out(
        generate_sql(
            opts["agg_type"],
            sql_string["probes_string"],
            sql_string.get("additional_queries", ""),
            sql_string.get("additional_partitions", ""),
            sql_string["select_clause"],
            sql_string.get("querying_table", "filtered"),
        )
    )


if __name__ == "__main__":
    main(sys.argv)
