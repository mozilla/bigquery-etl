---
version: 2.1

orbs:
  gcp-gcr: circleci/gcp-gcr@0.16
  docker: circleci/docker@2.6
  python: circleci/python@2.1.1
  gcp-cli: circleci/gcp-cli@3.2

parameters:
  python-version:
    type: string
    default: '3.11'
  validate-bqetl:
    type: boolean
    default: false
  validate-sql:
    type: boolean
    default: false
  validate-routines:
    type: boolean
    default: false
  deploy:
    type: boolean
    default: false
  trigger-sql-generation:
    type: boolean
    default: false
  skip-stage-deploys:
    type: boolean
    default: false

executors:
  ubuntu-machine-executor:
    machine:
      image: ubuntu-2004:current

jobs:
  build:
    docker: &docker
      - image: python:<< pipeline.parameters.python-version >>
    steps:
      - checkout
      - &restore_venv_cache
        restore_cache:
          keys:
            # when lock files change, use increasingly general
            # patterns to restore cache
            - &python_cache_key
              # yamllint disable-line rule:line-length
              python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-{{ checksum "requirements.in" }}-{{ checksum "requirements.txt" }}
            # yamllint disable-line rule:line-length
            - python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-{{ checksum "requirements.in" }}-
            # yamllint disable-line rule:line-length
            - python-<< pipeline.parameters.python-version >>-packages-v1-{{ .Branch }}-
            - python-<< pipeline.parameters.python-version >>-packages-v1-main-
      - &build
        run:
          name: Build
          command: |
            python3 -m venv venv/
            venv/bin/pip install --no-deps -r requirements.txt
            venv/bin/pip-sync --pip-args=--no-deps
      - save_cache:
          paths:
            - venv/
          key: *python_cache_key
  verify-format-sql:
    docker: *docker
    steps:
      - when:
          condition: &validate-sql-or-routines
            or:
              - << pipeline.parameters.validate-sql >>
              - << pipeline.parameters.validate-routines >>
              - << pipeline.parameters.deploy >>
          steps:
            - checkout
            - *restore_venv_cache
            - *build
            - run:
                name: Verify that SQL is correctly formatted
                command: |
                  PATH="venv/bin:$PATH" script/bqetl format --check \
                  $(git ls-tree -d HEAD --name-only)
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - &skip
              run:
                name: Skip
                command: echo "Skipped due to path filtering"
  verify-format-yaml:
    docker: *docker
    steps:
      - checkout
      - *restore_venv_cache
      - *build
      - run:
          name: Yamllint Test
          command: PATH="venv/bin:$PATH" yamllint -c .yamllint.yaml .
  verify-requirements:
    docker: *docker
    steps:
      - checkout
      - *restore_venv_cache
      - *build
      - run:
          name: Verify that requirements.txt contains the right dependencies for
            this python version
          command: |
            venv/bin/pip-compile --allow-unsafe --generate-hashes --quiet
            git diff --exit-code -G '^ *[^# ]' -- requirements.txt
  test-bqetl:
    docker: *docker
    steps:
      - when:
          condition: &validate-bqetl
            or:
              - << pipeline.parameters.validate-bqetl >>
              - << pipeline.parameters.deploy >>
          steps:
            - &skip_forked_pr
              run:
                name: Early return if this build is from a forked PR
                command: |
                  if [ -n "$CIRCLE_PR_NUMBER" ]; then
                    echo "Cannot pass creds to forked PRs," \
                      "so marking this step successful"
                    circleci-agent step halt
                  fi
            - checkout
            - *restore_venv_cache
            - *build
            - &authenticate
              run:
                name: Authenticate to GCP
                command: |
                  export GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcp.json"
                  echo 'export GOOGLE_APPLICATION_CREDENTIALS="/tmp/gcp.json"' >> "$BASH_ENV"
                  echo "$GCLOUD_SERVICE_KEY" > "$GOOGLE_APPLICATION_CREDENTIALS"
            - run:
                name: PyTest with linters
                # integration tests are run in a separate `integration` step;
                # SQL and routine tests are split out into a separate `test-sql` test
                # since those tests take the longest to run and running those tests
                # in parallel speeds up CI
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint --black --flake8 \
                    --isort --mypy-ignore-missing-imports --pydocstyle \
                    -m "not (routine or sql or integration)" \
                    -p no:bigquery_etl.pytest_plugin.routine \
                    -p no:bigquery_etl.pytest_plugin.sql \
                    -n 8
      - unless:
          condition: *validate-bqetl
          steps:
            - *skip
  test-sql:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - &attach_generated_sql
              attach_workspace:
                at: /tmp/workspace
            - &copy_staged_sql
              run:
                name: Move sql deployed on stage into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/staged-generated-sql/sql sql
                  rm -rf tests/
                  cp -r /tmp/workspace/staged-generated-sql/tests tests
            - run:
                name: Run SQL tests
                command: |
                    PATH="venv/bin:$PATH" script/entrypoint -m sql -n 8 -p no:bigquery_etl.pytest_plugin.routine
            - &copy_debug_sql
              run:
                name: Copy generated SQL to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r sql /tmp/debug_artifacts/sql
                when: on_fail
            - &copy_debug_tests
              run:
                name: Copy generated tests to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r tests /tmp/debug_artifacts/tests
                when: on_fail
            - &store_debug_artifacts
              store_artifacts:
                # Note that empty files are not stored by CircleCI
                path: /tmp/debug_artifacts
                destination: /
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  dry-run-sql:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Dry run queries
                # yamllint disable rule:line-length
                # Dry runs on PRs are executed on sql/bigquery-etl-integration-test
                # Artifacts (queries, views, UDFs) that are changed will be moved to the
                # bigquery-etl-integration-test folder and deployed to the corresponding
                # project. This ensures that dry runs can be executed before changes
                # have been deployed to prod. (bigquery-etl-integration-test is treated
                # as a stage environment)
                command: |
                  if [ "$CIRCLE_BRANCH" = main ]; then
                    echo "Check dry run for all queries because branch is" \
                      "$CIRCLE_BRANCH"
                    PATHS=sql
                  elif git log --format=%B --no-merges -n 1 |
                      grep -qF '[run-tests]'; then
                    echo "Check dry run for all queries because [run-tests] in" \
                      "commit message"
                    PATHS=sql
                  elif [ "<< pipeline.parameters.skip-stage-deploys >>" = "true" ]; then
                    PATHS=sql
                  else
                    PATHS="sql/bigquery-etl-integration-test"
                  fi
                  echo $PATHS
                  PATH="venv/bin:$PATH" script/bqetl dryrun --validate-schemas $PATHS
                # yamllint enable rule:line-length
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  validate-backfills:
    docker: *docker
    steps:
      - when:
          condition: &validate-sql
            or:
              - << pipeline.parameters.validate-sql >>
              - << pipeline.parameters.deploy >>
          steps:
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - &copy_generated_sql
              run:
                name: Move generated-sql into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/generated-sql/sql sql
            - run:
                name: Verify that backfill.yaml files are valid
                command: |
                  PATH="venv/bin:$PATH" script/bqetl backfill validate
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  validate-metadata:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - run:
                name: Validate workgroup access configuration on main
                command: |
                  PATH="venv/bin:$PATH" script/bqetl metadata validate-workgroups sql/
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Verify that metadata files are valid
                command: |
                  # TODO: Add check here to make sure all queries have metadata.yaml
                  PATH="venv/bin:$PATH" script/bqetl query validate \
                    --no-dryrun --skip-format-sql
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  integration:
    docker: *docker
    steps:
      - when:
          condition: *validate-bqetl
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - run:
                name: PyTest Integration Test
                # yamllint disable rule:line-length
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint -m 'integration' -n 8 \
                  -p no:bigquery_etl.pytest_plugin.routine \
                  -p no:bigquery_etl.pytest_plugin.sql
                # workaround for job failing with `Too long with no output (exceeded 10m0s): context deadline exceeded` error
                no_output_timeout: 30m
      - unless:
          condition: *validate-bqetl
          steps:
            - *skip
  generate-dags:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_generated_sql
            - *authenticate
            - run:
                name: Generate DAGs
                command: |
                  mkdir -p /tmp/workspace/generated-sql/dags
                  PATH="venv/bin:$PATH" script/bqetl dag generate --output-dir=/tmp/workspace/generated-sql/dags
            # this task is overwriting the content produced by generate-sql;
            # the behaviour here is additive, generated DAGs are just added to
            # the generated-sql output
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - generated-sql
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  validate-dags:
    executor:
      name: python/default
      tag: 3.11.8
    steps:
      - when:
          condition: *validate-sql
          steps:
            - checkout
            - run:
                name: Pull telemetry-airflow
                command: |
                  git clone https://github.com/mozilla/telemetry-airflow.git ~/telemetry-airflow
            - *attach_generated_sql
            - *copy_generated_sql
            - run:
                name: Replace telemetry-airflow DAGs with BigQuery ETL DAGs
                command: |
                  rm ~/telemetry-airflow/dags/* -f || true
                  cp -a /tmp/workspace/generated-sql/dags/. ~/telemetry-airflow/dags/
            - run:
                name: Install telemetry-airflow dependencies
                command: |
                  cd ~/telemetry-airflow
                  virtualenv .venv
                  source .venv/bin/activate
                  pip install -r requirements.txt
                  pip install -r requirements-dev.txt
                  pip install -r requirements-override.txt
            - run:
                name: ðŸ§ª Test valid DAGs
                command: |
                  cd ~/telemetry-airflow
                  source .venv/bin/activate
                  python -m pytest tests/dags/test_dag_validity.py --junitxml=~/telemetry-airflow/test-results/junit.xml
            - store_test_results:
                path: ~/telemetry-airflow/test-results/junit.xml
            - &copy_debug_dags
              run:
                name: Copy generated DAGs to save for debugging
                command: |
                  mkdir -p /tmp/debug_artifacts
                  cp -r dags /tmp/debug_artifacts/dags
                when: on_fail
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql
          steps:
            - *skip
  test-routines:
    docker: *docker
    steps:
      - when:
          condition: &validate-routines
            or:
              - << pipeline.parameters.validate-routines >>
              - << pipeline.parameters.deploy >>
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - run:
                name: Run routine tests
                command: |
                  PATH="venv/bin:$PATH" script/entrypoint -m routine -n 8
            - run:
                name: Validate doc examples
                command: |
                  PATH="venv/bin:$PATH" script/bqetl routine validate --docs-only
            - *copy_debug_sql
            - *copy_debug_tests
            - *store_debug_artifacts
      - unless:
          condition: *validate-routines
          steps:
            - *skip
  validate-views:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_staged_sql
            - *authenticate
            - run:
                name: Validate views
                command: |
                  PATH="venv/bin:$PATH" script/bqetl view validate
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  docs:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Build and deploy docs
                command: |
                  rm -r sql/ && cp -r /tmp/workspace/generated-sql/sql sql/
                  PATH="venv/bin:$PATH" script/bqetl docs generate \
                    --output_dir=generated_docs/
                  cd generated_docs/
                  PATH="../venv/bin:$PATH" mkdocs gh-deploy \
                    -m "[ci skip] Deployed {sha} with MkDocs version: {version}"
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  generate-sql:
    docker: *docker
    resource_class: large
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *authenticate
            - run:
                name: Generate SQL content
                command: |
                  # Get commit on main from which this branch diverged
                  MAIN_MERGE_BASE=$(git merge-base main HEAD || true)
                  echo "merge base with main: $MAIN_MERGE_BASE"

                  # Check if the generated-sql branch can simply be re-used and SQL generation can be skipped.
                  git clone -b generated-sql git@github.com:mozilla/bigquery-etl ~/remote-generated-sql
                  cd ~/remote-generated-sql

                  # check if main merge base has been pushed to generated-sql yet, otherwise use latest
                  if [[ -n $MAIN_MERGE_BASE ]] && git show-ref --tags c-${MAIN_MERGE_BASE:0:9} --quiet; then
                    echo "Tag c-${MAIN_MERGE_BASE:0:9} exists in generated sql"
                    git checkout tags/c-${MAIN_MERGE_BASE:0:9}
                  else
                    # Commits on generated-sql are tagged with the sha1 hash of the commit on main that triggered the SQL generation
                    export LATEST_TAG_GENERATED_SQL=`git describe --tags --abbrev=0`
                    LATEST_TAG_GENERATED_SQL=${LATEST_TAG_GENERATED_SQL/c-/""}
                    echo "Latest tag on generated-sql branch: c-$LATEST_TAG_GENERATED_SQL"
                  fi

                  # Get the state in which main was in when the generated-sql branch was created and pushed
                  cd ~
                  git clone -b main git@github.com:mozilla/bigquery-etl ~/remote-bigquery-etl-main
                  cd ~/remote-bigquery-etl-main
                  export LATEST_COMMIT_ON_MAIN=`git rev-parse --short HEAD`
                  git checkout $LATEST_TAG_GENERATED_SQL
                  echo "Latest commit on main: $LATEST_COMMIT_ON_MAIN"

                  if [[ -n $MAIN_MERGE_BASE ]]; then
                    echo "Checking out $MAIN_MERGE_BASE for diff"
                    git checkout $MAIN_MERGE_BASE
                  fi

                  cd ~/project
                  export GENERATED_SQL_CHANGED=false

                  # Manually trigger SQL generation via a pipeline parameter
                  if [ "<< pipeline.parameters.trigger-sql-generation >>" = "true" ]; then
                    echo "trigger-sql-generation was set to true; run SQL generation"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If this task runs when merging into main, then always run the SQL generation
                  if [ "$CIRCLE_BRANCH" = main ]; then
                    echo "On main branch; generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # Check if this PR removes or renames a file. SQL generation needs to be re-run because
                  # simply merging the changes with the generated-sql branch would result in files being part of generated DAGs
                  # and other artifacts that were actually removed or renamed
                  if [[ $(git diff --no-index --name-only --diff-filter=DR ~/remote-bigquery-etl-main/sql sql) ]]; then
                    echo "SQL file was removed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # Check if this PR changes SQL generators
                  if [[ $(diff -qr --no-dereference sql_generators ~/remote-bigquery-etl-main/sql_generators) ]]; then
                    echo "SQL generators changed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  mkdir -p /tmp/workspace/generated-sql

                  if [ "$GENERATED_SQL_CHANGED" = "true" ]; then
                    echo "Changes made will affect generated SQL. Run SQL generators."

                    cp -r sql/ /tmp/workspace/generated-sql/sql
                    # Don't depend on dry run for PRs
                    PATH="venv/bin:$PATH" script/bqetl generate all \
                      --output-dir /tmp/workspace/generated-sql/sql/ \
                      --target-project moz-fx-data-shared-prod
                  else
                    echo "Changes made don't affect generated SQL. Use content from generated-sql"

                    # merge files that changed in this PR with the generated-sql branch
                    cp -r ~/remote-generated-sql/sql/ /tmp/workspace/generated-sql/sql
                    cp -a sql/. /tmp/workspace/generated-sql/sql
                  fi

                  PATH="venv/bin:$PATH" script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/generated-sql/sql/"
                  PATH="venv/bin:$PATH" script/bqetl metadata update \
                    --sql-dir /tmp/workspace/generated-sql/sql/ \
                    /tmp/workspace/generated-sql/sql/
                  PATH="venv/bin:$PATH" script/bqetl format /tmp/workspace/generated-sql/sql/
                  PATH="venv/bin:$PATH" script/bqetl monitoring update /tmp/workspace/generated-sql/sql/
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - generated-sql
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  deploy-changes-to-stage:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *attach_generated_sql
            - *copy_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Pull in generated-sql branch from remote
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git clone --single-branch --branch generated-sql \
                    git@github.com:mozilla/bigquery-etl \
                    generated-sql
            - *authenticate
            - run:
                name: Deploy changes to stage
                command: |
                  if [ "<< pipeline.parameters.skip-stage-deploys >>" = "false" ]; then
                    PATHS="$(git diff --no-index --name-only --diff-filter=d generated-sql/sql sql)" || true
                    echo $PATHS
                    PATH="venv/bin:$PATH" script/bqetl stage deploy \
                      --dataset-suffix=$CIRCLE_SHA1 \
                      --remove-updated-artifacts \
                      $PATHS
                  fi
                # workaround for job failing with `Too long with no output (exceeded 10m0s): context deadline exceeded` error
                no_output_timeout: 30m
            - run:
                name: Copy generated SQL to temporary stage directory
                command: |
                  mkdir -p /tmp/workspace/staged-generated-sql
                  cp -r sql/ /tmp/workspace/staged-generated-sql/sql
                  cp -r tests/ /tmp/workspace/staged-generated-sql/tests
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - staged-generated-sql
            - *copy_debug_sql
            - *store_debug_artifacts
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  push-generated-sql:
    docker: *docker
    steps:
      - when:
          condition: &deploy
            or:
              - << pipeline.parameters.deploy >>
          steps:
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "22:b9:3c:1b:82:ab:3f:e4:b5:79:70:d1:7b:b9:28:d2"
            - run:
                name: Push to generated-sql branch
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.name "CircleCI generate-sql job"
                  git config --global user.email "dataops+generated-sql@mozilla.com"
                  git clone --single-branch --branch generated-sql \
                    git@github.com:mozilla/bigquery-etl \
                    generated-sql
                  cd generated-sql/
                  rm -rf sql/
                  cp -r /tmp/workspace/generated-sql/sql sql
                  rm -rf dags/
                  cp -r /tmp/workspace/generated-sql/dags dags
                  git add .
                  git commit -m "Auto-push due to change on main branch [ci skip]" \
                    && git tag c-${CIRCLE_SHA1:0:9} \
                    && git push \
                    && git push origin c-${CIRCLE_SHA1:0:9} \
                    || echo "Skipping push since it looks like there were no changes"
      - unless:
          condition: *deploy
          steps:
            - *skip
  sync-dags-repo:
    executor: docker/machine
    parameters:
      repo-to-sync:
        type: string
      target-branch:
        type: string
    steps:
      - when:
          condition: *deploy
          steps:
            - checkout
            - add_ssh_keys:
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
                  - "e4:30:50:41:53:f0:d6:3a:bb:c9:38:54:2d:ca:56:41"
            - run:
                name: ðŸ¤– Update DAGs repository
                # ~/.ssh/config needs to be manually configured to support cloning a private
                # repository through submodules and writing to another repository.
                command: |
                  cat \<<'EOT' > ~/.ssh/config
                  Host github.com
                    IdentitiesOnly yes
                    IdentityFile /home/circleci/.ssh/id_rsa_9d1eaf52782ce8ec334cdbcd5aff700a
                  Host git-telemetry-airflow-dags
                    HostName github.com
                    User git
                    IdentitiesOnly yes
                    IdentityFile /home/circleci/.ssh/id_rsa_e430504153f0d63abbc938542dca5641
                  EOT
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.email "dataops@mozilla.com"
                  git config --global user.name "CircleCI Job"
                  git clone --branch main git@git-telemetry-airflow-dags:mozilla/telemetry-airflow-dags.git
                  cd ~/project/telemetry-airflow-dags
                  git submodule update --init --recursive --depth 1
                  cd << parameters.repo-to-sync >>
                  git pull origin << parameters.target-branch >>
                  cd ..
                  git add << parameters.repo-to-sync >>
                  git commit --allow-empty -m "Automatic commit from ${CIRCLE_PROJECT_REPONAME} commit ${CIRCLE_SHA1:0:9} build ${CIRCLE_BUILD_NUM} [skip ci]"
                  git push origin main
      - unless:
          condition: *deploy
          steps:
            - *skip
  artifact-deployment:
    executor: gcp-cli/google
    steps:
      - run:
          name: Prepare environment variables for OIDC authentication
          # Project ID is not used for OIDC authentication, but gcloud CLI requires a valid project ID.
          command: |
            echo 'export GOOGLE_PROJECT_ID="moz-fx-telemetry-airflow-prod"' >> "$BASH_ENV"
            echo "export OIDC_WIP_ID=$GCPV2_WORKLOAD_IDENTITY_POOL_ID" >> "$BASH_ENV"
            echo "export OIDC_WIP_PROVIDER_ID=$GCPV2_CIRCLECI_WORKLOAD_IDENTITY_PROVIDER" >> "$BASH_ENV"
            echo "export GOOGLE_PROJECT_NUMBER=$GCPV2_WORKLOAD_IDENTITY_POOL_PROJECT_NUMBER" >> "$BASH_ENV"
            echo "export OIDC_SERVICE_ACCOUNT_EMAIL=$GCP_SERVICE_ACCOUNT_EMAIL" >> "$BASH_ENV"
      - gcp-cli/setup:
          use_oidc: true
      - run:
          name: Generate API Token and DAG run note
          command: |
            echo "export DAGRUN_NOTE=\"DAG triggered by **[${CIRCLE_USERNAME}](https://github.com/${CIRCLE_USERNAME})** from ${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME} CI build [${CIRCLE_BUILD_NUM}](${CIRCLE_BUILD_URL})\"" >> "$BASH_ENV"
            echo "export ID_TOKEN=$(gcloud auth print-identity-token --impersonate-service-account ${GCP_SERVICE_ACCOUNT_EMAIL})" >> "$BASH_ENV"
      - run:
          name: Trigger bqetl-artifact-deployment in Airflow
          command: >
            curl --location --request POST "https://us-west1-moz-fx-telemetry-airflow-prod.cloudfunctions.net/ci-external-trigger"
            -H "Authorization: bearer ${ID_TOKEN}"
            -H "Content-Type:application/json"
            -d "{\"dagrun_note\": \"${DAGRUN_NOTE}\", \"dag_id\": \"bqetl_artifact_deployment\"}"
  private-generate-sql:
    docker: *docker
    resource_class: large
    steps:
      - when:
          condition: *deploy
          steps:
            - *skip_forked_pr
            - checkout
            - *restore_venv_cache
            - *build
            - *authenticate
            - add_ssh_keys:
                # deploy key to private-bigquery-etl
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
            - run:
                name: Install rsync
                command: |
                  apt update
                  apt install -y rsync
            - run:
                name: Pull down private SQL content
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git clone --single-branch --branch main \
                                git@github.com:mozilla/private-bigquery-etl.git \
                                ~/private-bigquery-etl
                  rsync --archive ~/private-bigquery-etl/sql/ sql/
                  rsync --archive ~/private-bigquery-etl/dags.yaml dags.yaml
            - run:
                name: Generate SQL content
                no_output_timeout: 30m
                command: |
                  mkdir -p /tmp/workspace/private-generated-sql

                  cp -r sql/ /tmp/workspace/private-generated-sql/sql
                  # Don't depend on dry run for PRs
                  PATH="venv/bin:$PATH" script/bqetl generate all \
                    --output-dir /tmp/workspace/private-generated-sql/sql/ \
                    --target-project moz-fx-data-shared-prod
                  PATH="venv/bin:$PATH" script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/private-generated-sql/sql/"
                  PATH="venv/bin:$PATH" script/bqetl metadata update \
                    --sql-dir /tmp/workspace/private-generated-sql/sql/ \
                    /tmp/workspace/private-generated-sql/sql/
                  PATH="venv/bin:$PATH" script/bqetl format /tmp/workspace/private-generated-sql/sql/
                  PATH="venv/bin:$PATH" script/bqetl monitoring update /tmp/workspace/private-generated-sql/sql/

                  # Change directory to generate DAGs so `sql_file_path` values are relative to the repo root.
                  export PATH="$PWD/venv/bin:$PATH"
                  export PYTHONPATH="$PWD:$PYTHONPATH"
                  bqetl_script="$PWD/script/bqetl"
                  cd ~/private-bigquery-etl
                  mkdir -p /tmp/workspace/private-generated-sql/dags
                  $bqetl_script dag generate \
                    --output-dir /tmp/workspace/private-generated-sql/dags/
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - private-generated-sql
      - unless:
          condition: *deploy
          steps:
            - *skip
  push-private-generated-sql:
    docker: *docker
    steps:
      - when:
          condition: *deploy
          steps:
            - *attach_generated_sql
            - add_ssh_keys:
                fingerprints:
                  - "9d:1e:af:52:78:2c:e8:ec:33:4c:db:cd:5a:ff:70:0a"
            - run:
                name: Push to private-generated-sql branch
                # yamllint disable rule:line-length
                command: |
                  ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
                  git config --global user.name "CircleCI private-generate-sql job"
                  git config --global user.email "dataops+private-generated-sql@mozilla.com"
                  git clone --single-branch --branch private-generated-sql \
                    git@github.com:mozilla/private-bigquery-etl \
                    private-generated-sql
                  cd private-generated-sql/
                  rm -rf sql/
                  cp -r /tmp/workspace/private-generated-sql/sql sql
                  rm -rf dags/
                  cp -r /tmp/workspace/private-generated-sql/dags dags
                  git add .
                  git commit -m "Auto-push due to change on main branch [ci skip]" \
                    && git push \
                    || echo "Skipping push since it looks like there were no changes"
                # yamllint enable rule:line-length
      - unless:
          condition: *deploy
          steps:
            - *skip
  deploy-to-private-gcr:
    executor: ubuntu-machine-executor
    steps:
      - when:
          condition: *deploy
          steps:
            - checkout
            - *attach_generated_sql
            - run:
                name: Move generated-sql into place
                command: |
                  rm -rf sql/
                  cp -r /tmp/workspace/private-generated-sql/sql sql
            - gcp-gcr/gcr-auth:
                use_oidc: true
            - gcp-gcr/build-image: &private-image
                image: bigquery-etl
                tag: ${CIRCLE_TAG:-latest}
            - gcp-gcr/push-image: *private-image
      - unless:
          condition: *deploy
          steps:
            - *skip
  main-generate-sql-and-dags:
    docker: *docker
    resource_class: large
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - run:
                name: Switch to main branch
                command: |
                  git remote add mozilla git@github.com:mozilla/bigquery-etl
                  git fetch mozilla main
                  git checkout mozilla/main
            - attach_workspace:
                at: /tmp/workspace
            - *restore_venv_cache
            - *build
            - *authenticate
            - run:
                name: Generate SQL content
                command: |
                  export PATH="venv/bin:$PATH"

                  # Get parent commit on main from which this branch diverged
                  MAIN_MERGE_BASE=$(git merge-base HEAD $CIRCLE_BRANCH || true)
                  echo "merge base with main: $MAIN_MERGE_BASE"

                  # Check if the generated-sql branch can simply be re-used and SQL generation can be skipped.
                  # There is a delay between pushing changes to main and pushing a new generated-sql branch,
                  # so we need to account for possible changes that happen during this time
                  git clone -b generated-sql git@github.com:mozilla/bigquery-etl ~/remote-generated-sql
                  cd ~/remote-generated-sql

                  # check if main merge base has been pushed to generated-sql yet, otherwise use latest
                  if [[ -n $MAIN_MERGE_BASE ]] && git show-ref --tags c-${MAIN_MERGE_BASE:0:9} --quiet; then
                    echo "Tag c-${MAIN_MERGE_BASE:0:9} exists in generated sql"
                    git checkout tags/c-${MAIN_MERGE_BASE:0:9}
                  else
                    # Commits on generated-sql are tagged with the sha1 hash of the commit on main that triggered the SQL generation
                    export LATEST_TAG_GENERATED_SQL=`git describe --tags --abbrev=0`
                    LATEST_TAG_GENERATED_SQL=${LATEST_TAG_GENERATED_SQL/c-/""}
                    echo "Latest tag on generated-sql branch: c-$LATEST_TAG_GENERATED_SQL"
                  fi

                  cd ~/project
                  export GENERATED_SQL_CHANGED=false

                  if [[ -n $MAIN_MERGE_BASE ]]; then
                    echo "Checking out $MAIN_MERGE_BASE for diff"
                    git checkout $MAIN_MERGE_BASE
                  fi

                  # Manually trigger SQL generation via a pipeline parameter
                  if [ "<< pipeline.parameters.trigger-sql-generation >>" = "true" ]; then
                    echo "trigger-sql-generation was set to true; run SQL generation"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If the between main the generated-sql branch SQL generators have changed, then re-run the SQL generation
                  if [[ $(git diff --name-only --diff-filter=d $LATEST_TAG_GENERATED_SQL...HEAD -- sql_generators) ]]; then
                    echo "SQL generator changed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  # If a SQL was removed from main or renamed re-run SQL generation
                  # simply merging the changes with the generated-sql branch would result in files being part of generated DAGs
                  # and other artifacts that were removed or renamed
                  if [[ $(git diff --name-only --diff-filter=DR $LATEST_TAG_GENERATED_SQL...HEAD -- sql) ]]; then
                    echo "SQL file was removed; re-generate SQL"
                    GENERATED_SQL_CHANGED=true
                  fi

                  mkdir -p /tmp/workspace/main-generated-sql
                  mkdir -p /tmp/workspace/main-generated-sql/dags

                  if [ "$GENERATED_SQL_CHANGED" = "true" ]; then
                    # re-run SQL generation and re-generate DAGs
                    ./script/bqetl generate all \
                      --target-project moz-fx-data-shared-prod

                    cp -r sql/ /tmp/workspace/main-generated-sql/sql
                  else
                    echo "Changes made don't affect generated SQL. Use content from generated-sql"

                    # merge files that changed in this PR with the generated-sql branch
                    cp -r ~/remote-generated-sql/sql/. /tmp/workspace/main-generated-sql/sql
                    cp -a sql/. /tmp/workspace/main-generated-sql/sql
                  fi

                  ./script/bqetl dependency record \
                    --skip-existing \
                    "/tmp/workspace/main-generated-sql/sql/"
                  ./script/bqetl metadata update \
                    --sql-dir /tmp/workspace/main-generated-sql/sql/ \
                    /tmp/workspace/main-generated-sql/sql/
                  # re-generate DAGs
                  ./script/bqetl format /tmp/workspace/main-generated-sql/sql/

                  ./script/bqetl dag generate \
                    --sql-dir /tmp/workspace/main-generated-sql/sql/ \
                    --output-dir /tmp/workspace/main-generated-sql/dags

                  find /tmp/workspace/main-generated-sql/dags -name '*.py' \
                    -exec sed -i -e 's/\/tmp\/workspace\/main-generated-sql\/sql/sql/g' {} \;
            - persist_to_workspace:
                root: /tmp/workspace
                paths:
                  - main-generated-sql
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  generate-diff:
    docker:
      - image: cimg/node:21.2.0
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - checkout
            - attach_workspace:
                at: /tmp/workspace
            - run:
                name: Generate diff
                command: |
                  diff -qr --no-dereference \
                    /tmp/workspace/main-generated-sql/dags/ /tmp/workspace/generated-sql/dags/ \
                    | grep '^Only in' \
                    > /tmp/workspace/sql.diff || true
                  diff -bur --no-dereference --new-file \
                    /tmp/workspace/main-generated-sql/dags/ /tmp/workspace/generated-sql/dags/ \
                    >> /tmp/workspace/sql.diff || true
                  diff -qr --no-dereference \
                    /tmp/workspace/main-generated-sql/sql/ /tmp/workspace/generated-sql/sql/ \
                    | grep '^Only in' \
                    >> /tmp/workspace/sql.diff || true
                  diff -bur --no-dereference --new-file \
                    /tmp/workspace/main-generated-sql/sql/ /tmp/workspace/generated-sql/sql/ \
                    >> /tmp/workspace/sql.diff || true
            - store_artifacts:
                path: /tmp/workspace/sql.diff
                destination: sql.diff
            - run: npm i circle-github-bot@2.1.0 @octokit/graphql@7.0.2
            - run: .circleci/post-diff.js
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  reset-stage-env:
    docker: *docker
    steps:
      - when:
          condition: *validate-sql-or-routines
          steps:
            - *skip_forked_pr
            - checkout
            - *build
            - *authenticate
            - run:
                name: "Delete stage datasets"
                command: |
                  PATH="venv/bin:$PATH" script/bqetl stage clean --dataset-suffix=$CIRCLE_SHA1 --delete-expired
      - unless:
          condition: *validate-sql-or-routines
          steps:
            - *skip
  manual-trigger-required-for-fork:
    docker: *docker
    steps:
      - &skip_upstream
        run:
          name: Early return if this build is running on upstream
          command: |
            if [ -n "$CIRCLE_PR_NUMBER" ]; then
              echo "Build on fork"
            else
              echo "Build on upstream"
              circleci-agent step halt
            fi
      - checkout
      - run:
          name: Manually trigger integration tests for fork
          # yamllint disable rule:line-length
          command: |
            apt update
            apt install jq -y

            CIRCLE_PR_BRANCH=`curl -s https://api.github.com/repos/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/pulls/${CIRCLE_PR_NUMBER} | jq -r '.head.label'`

            echo "Integration tests for this fork need to be triggered manually"
            echo "Users with write access to the repository can trigger" \
              "integration tests by following these steps: "
            echo "  Open the following page:"
            echo "    https://github.com/mozilla/bigquery-etl/actions/workflows/push-to-upstream.yml"
            echo "  Choose the 'Run workflow' dropdown and provide '$CIRCLE_PR_BRANCH' as parameter."

            exit 1
          # yamllint enable rule:line-length
  deploy-to-pypi:
    docker: *docker
    steps:
      - checkout
      - run:
          name: Check for package version change in last commit before proceeding.
          command: |
            if git diff main HEAD~1 pyproject.toml | grep 'version'
              then
                echo "Found changes to package version dir, proceeding with deployment."
              else
                echo "No changes in package version. Skipping bigquery-etl deployment."
                circleci-agent step halt
            fi
      - run:
          name: Install deployment tools
          command: |
            pip install --upgrade setuptools wheel twine
      - run:
          name: Create the distribution files
          command: |
            python -m pip install build
            python -m build --sdist --wheel
      - run:
          name: Upload to PyPI
          command: |
            # Relies on the TWINE_USERNAME and TWINE_PASSWORD environment variables
            # For more on twine, see:
            #   https://twine.readthedocs.io/en/latest/
            twine upload --skip-existing dist/*

workflows:
  version: 2
  build:
    jobs: &build_jobs
      - manual-trigger-required-for-fork
      - build
      - verify-requirements
      - verify-format-yaml
      - verify-format-sql
      - deploy-changes-to-stage:
          requires:
            - generate-sql
      - dry-run-sql:
          requires:
            - deploy-changes-to-stage
      - test-sql:
          requires:
            - deploy-changes-to-stage
      - validate-metadata:
          requires:
            - deploy-changes-to-stage
      - validate-backfills:
          requires:
            - generate-sql
      - validate-dags:
          requires:
            - generate-dags
      - validate-views:
          requires:
            - deploy-changes-to-stage
      - generate-sql
      - main-generate-sql-and-dags:
          filters:
            branches:
              ignore: main
      - generate-diff:
          requires:
            - generate-dags
            - main-generate-sql-and-dags
          filters:
            branches:
              ignore: main
      - generate-dags:
          requires:
            - generate-sql
      - docs:
          requires:
            - generate-sql
          filters:
            branches:
              only: main
      - push-generated-sql:
          requires:
            - validate-dags
          filters:
            branches:
              only:
                - main
      - reset-stage-env:
          requires:
            - push-generated-sql
            - test-sql
            - validate-views
            - validate-metadata
            - dry-run-sql
      - test-routines:
          requires:
            - deploy-changes-to-stage
      - test-bqetl
      - integration
      # The following "private" jobs are basically clones of the public jobs
      # for generate-sql, deploy, and push-generated-sql, except that they pull
      # in some additional content from an internal Mozilla repository for
      # cases where ETL code cannot be public. Although the CI logic is
      # consolidated in this public repository, note that we are both pulling
      # from the internal repository and pushing generated results back to
      # a branch on that internal repository, which may be initially
      # surprising.
      - private-generate-sql:
          filters:
            branches:
              only:
                - main
      - push-private-generated-sql:
          requires:
            - private-generate-sql
          filters:
            branches:
              only:
                - main
      - artifact-deployment:
          name: Trigger bqetl_artifact_deployment Airflow DAG
          context: gcpv2-workload-identity
          requires:
            - deploy-to-private-gcr
          filters:
            branches:
              only:
                - main
      - sync-dags-repo:
          name: ðŸ”ƒ Synchronize bigquery-etl submodule
          repo-to-sync: ${CIRCLE_PROJECT_REPONAME}
          target-branch: generated-sql
          requires:
            - push-generated-sql
          filters:
            branches:
              only:
                - main
      - sync-dags-repo:
          name: ðŸ”ƒ Synchronize private-bigquery-etl submodule
          repo-to-sync: private-bigquery-etl
          target-branch: private-generated-sql
          requires:
            - push-private-generated-sql
            - ðŸ”ƒ Synchronize bigquery-etl submodule
          filters:
            branches:
              only:
                - main
      - deploy-to-private-gcr:
          context: dataeng-bqetl-gcr
          requires:
            - private-generate-sql
            # can't run in parallel because CIRCLE_BUILD_NUM is same
            - build
            - generate-sql
          filters:
            branches:
              only:
                - main
  tagged-deploy:
    jobs:
      - deploy-to-pypi:
          filters:
            branches:
              only:
                - main
