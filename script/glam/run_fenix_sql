#!/bin/bash
# Run the scalars sql job against all Glean pings in a product. Assumes that
# generate_fenix_sql has already been run.

set -e

# `date` is not consistent across MacOS and GNU/Linux
function yesterday {
    python3 - <<EOD
from datetime import date, timedelta
dt = date.today() - timedelta(1)
print(dt.strftime("%Y-%m-%d"))
EOD
}

PROJECT="glam-fenix-dev"
PROD_DATASET="glam_etl"
DATASET=${DATASET:-"glam_etl_dev"}
SUBMISSION_DATE=${SUBMISSION_DATE:-$(yesterday)}


# Replace PROD_DATASET with DATASET in order to run the entire pipeline within a
# separate dataset. This is effectively a hack for what should be generated
# queries in the correct location. However, this provides a mechanism for
# changing the dataset location for testing.
function replace_dataset {
    local sql_path=$1
    sed "s/$PROD_DATASET/$DATASET/g" < "$sql_path"
}


# replace project and dataset in an init or view file
function replace_project_dataset {
    local sql_path=$1
    local prod="moz-fx-data-shared-prod"
    sed "s/$prod/$PROJECT/g" < "$sql_path" | \
        sed "s/$PROD_DATASET/$DATASET/g"
}


function run_query {
    local destination_table=$1
    local time_partition=${2:-false}
    local query="sql/glam_etl/$destination_table/query.sql"
    # add an option to write to a time-partitioned table
    if $time_partition; then
        destination_table="${destination_table}\$${SUBMISSION_DATE//-/}"
    fi
    echo "running $query"
    local tmp
    tmp=$(mktemp)
    replace_dataset "$query" > "$tmp"
    bq query \
        --max_rows=0 \
        --use_legacy_sql=false \
        --replace \
        --project_id="$PROJECT" \
        --dataset_id="$DATASET" \
        --destination_table="$destination_table" \
        --parameter="submission_date:DATE:$SUBMISSION_DATE" \
        --parameter="min_sample_id:INT64:0" \
        --parameter="max_sample_id:INT64:99" \
        "$(if $time_partition; then echo --time_partitioning_type="DAY"; fi)" \
        < "$tmp"
}


function run_init {
    local destination_table=$1
    local init="sql/glam_etl/$destination_table/init.sql"
    # run if needed
    if ! bq show --quiet "${DATASET}.${destination_table}"; then
        echo "running $init"
        local tmp
        tmp=$(mktemp)
        replace_project_dataset "$init" > "$tmp"
        bq query \
            --use_legacy_sql=false \
            --project_id="$PROJECT" \
            < "$tmp"
    fi
}


function run_view {
    local view_name=$1
    local view="sql/glam_etl/$view_name/view.sql"
    echo "running $view"
    local tmp
    tmp=$(mktemp)
    replace_project_dataset "$view" > "$tmp"
    bq query \
        --use_legacy_sql=false \
        --project_id="$PROJECT" \
        --dataset_id="$DATASET" \
        < "$tmp"
}


function main {
    cd "$(dirname "$0")/../.."

    local start_stage=${START_STAGE:-0}
    local reset=${RESET:-false}

    # revert to the original default project in the environment
    original_project=$(gcloud config get-value project)
    function cleanup {
        gcloud config set project "$original_project"
    }
    trap cleanup EXIT

    gcloud config set project $PROJECT

    # force delete the dataset
    if $reset; then
        bq rm -r -f "$DATASET"
    fi
    if ! bq ls "${PROJECT}:${DATASET}"; then
        bq mk "$DATASET"
    fi

    if ((start_stage <= 0)); then
        for directory in sql/glam_etl/fenix_clients_daily_scalar_aggregates*/; do
            run_query "$(basename "$directory")" true
        done
        for directory in sql/glam_etl/fenix_clients_daily_histogram_aggregates*/; do
            run_query "$(basename "$directory")" true
        done
    fi
    # NOTE: there isn't a mechanism to test the incremental query here
    if ((start_stage <= 1)); then
        run_query "fenix_latest_versions_v1"

        run_view "fenix_view_clients_daily_scalar_aggregates_v1"
        run_init "fenix_clients_scalar_aggregates_v1"
        run_query "fenix_clients_scalar_aggregates_v1"
    fi
    if ((start_stage <= 2)); then
        run_query "fenix_clients_scalar_bucket_counts_v1"
        run_query "fenix_clients_scalar_probe_counts_v1"
        run_query "fenix_scalar_percentiles_v1"
    fi
    if ((start_stage <= 3)); then
        run_view "fenix_view_clients_daily_histogram_aggregates_v1"
        run_init "fenix_clients_histogram_aggregates_v1"
        run_query "fenix_clients_histogram_aggregates_v1"
    fi
    if ((start_stage <= 4)); then
        run_query "fenix_clients_histogram_bucket_counts_v1"
        run_query "fenix_clients_histogram_probe_counts_v1"
        # TODO
        # run_query "fenix_histogram_percentiles_v1"
    fi
}

main
