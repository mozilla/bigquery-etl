#!/usr/bin/env python3

"""
Export query result data as JSON to a publicly accessible bucket.

Data of the query is exported if "json_export" is set in
the corresponding metadata file.
"""

from argparse import ArgumentParser
import os
import sys
import re

from google.cloud import bigquery

# sys.path needs to be modified to enable package imports from parent
# and sibling directories. Also see:
# https://stackoverflow.com/questions/6323860/sibling-package-imports/23542795#23542795
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from bigquery_etl.parse_metadata import Metadata  # noqa E402


DEFAULT_BUCKET = "ascholtz-dev-test"  # todo
METADATA_FILE = "metadata.yaml"
SUBMISSION_DATE_RE = re.compile(r"^submission_date:DATE:(\d\d\d\d-\d\d-\d\d)$")
QUERY_FILE_RE = re.compile(r"^.*/([a-zA-Z0-9_]+)/([a-zA-Z0-9_]+)_(v[0-9]+)/query\.sql$")

parser = ArgumentParser(description=__doc__)
parser.add_argument(
    "--target-bucket",
    "--target_bucket",
    default=DEFAULT_BUCKET,
    help="GCP bucket JSON data is exported to.",
)
parser.add_argument(
    "--project_id",
    default="moz-fx-data-derived-datasets",
    help="Run query in the target project",
)
parser.add_argument("--parameter", help="Query parameters, such as submission_date")
parser.add_argument("--query_file", help="File path to query to be executed")


def publish_table_as_json(
    bucket, client, dataset, table, version, result_table, date=None
):
    """Export the `result_table` data as JSON to Cloud Storage."""
    # "*" makes sure that files larger than 1GB get split up into multiple JSON files
    destination_uri = f"gs://{bucket}/{dataset}/{table}/{version}/files/"

    if date is None:
        # if date is not set, then export the entire table and overwrite existing JSON
        destination_uri += "*.json"
    else:
        # if date exists, then query is incremental and newest results are exported
        destination_uri += f"{date}/*.json"

    table_ref = client.get_table(result_table)

    job_config = bigquery.ExtractJobConfig()
    job_config.destination_format = "NEWLINE_DELIMITED_JSON"
    extract_job = client.extract_table(
        table_ref, destination_uri, location="US", job_config=job_config
    )
    extract_job.result()


def write_results_to_temp_table(client, project, query_file, date):
    """
    Write the results of the query to a temporary table and return the table
    name.
    """
    (dataset, table, version) = dataset_table_version_from_file(query_file)

    table_date = date.replace("-", "")
    temp_table = f"{project}.{dataset}.{table}_{version}_{table_date}_temp"

    with open(query_file) as query_stream:
        sql = query_stream.read()
        job_config = bigquery.QueryJobConfig(destination=temp_table)
        query_job = client.query(sql, job_config=job_config)
        query_job.result()

        return temp_table


def dataset_table_version_from_file(file_name):
    """Extract the dataset, table and version from the provided file name."""

    query_file_re = re.search(QUERY_FILE_RE, file_name)
    if query_file_re:
        dataset = query_file_re.group(1)
        table = query_file_re.group(2)
        version = query_file_re.group(3)

        return (dataset, table, version)
    else:
        print("Invalid file naming format: {}", file_name)
        return None


def main():
    args, query_arguments = parser.parse_known_args()

    try:
        metadata = Metadata.of_sql_file(args.query_file)
    except FileNotFoundError:
        print("No metadata file for: {}".format(args.query_file))
        return

    # check if the data should be published as JSON
    if not metadata.is_public_json():
        return

    client = bigquery.Client(args.project_id)
    date_search = re.search(SUBMISSION_DATE_RE, args.parameter)

    if date_search:
        date = date_search.group(1)

    (dataset, table, version) = dataset_table_version_from_file(args.query_file)

    if metadata.is_incremental():
        if date is not None:
            # if it is an incremental query, then the query result needs to be
            # written to a temporary table to get exported as JSON
            temp_result_table = write_results_to_temp_table(
                client, args.project_id, args.query_file, date
            )
            publish_table_as_json(
                args.target_bucket,
                client,
                dataset,
                table,
                version,
                temp_result_table,
                date,
            )

            # remove the temporary table after it has been exported
            client.delete_table(temp_result_table)
        else:
            print("Cannot publish JSON. submission_date needs to be set as parameter.")
            return
    else:
        # for non-incremental queries, the entire destination table is exported
        result_table = f"{dataset}.{table}_{version}"
        publish_table_as_json(
            args.target_bucket, client, dataset, table, version, result_table
        )


if __name__ == "__main__":
    main()
