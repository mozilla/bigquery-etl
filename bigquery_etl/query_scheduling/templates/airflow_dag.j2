# Generated via query_scheduling/generate_airflow_dags 

from airflow import DAG
from airflow.operators.sensors import ExternalTaskSensor
import datetime
from utils.gcp import bigquery_etl_query

default_args = {{ 
    default_args.to_dict() | 
    format_attr("start_date", "format_date") | 
    format_attr("retry_delay", "format_timedelta") 
}}

with DAG('{{ name }}', default_args=default_args, schedule_interval={{ schedule_interval | format_timedelta | format_schedule_interval }}) as dag:
{% for task in tasks %}
    {{ task.task_name }} = bigquery_etl_query(
        destination_table='{{ task.table }}',
        dataset_id='{{ task.dataset }}',
        project_id='moz-fx-data-shared-prod',
        owner='{{ task.owner }}',
        {%+ if task.email | length > 0 -%}
        email={{ task.email }},
        {%+ endif -%}
        {%+ if task.start_date -%}
        start_date={{ task.start_date | format_date }},
        {%+ endif -%}
        {%+ if task.date_partition_parameter == None or task.date_partition_parameter is string -%}
        date_partition_parameter={{ task.date_partition_parameter | format_optional_string }},
        {%+ endif -%}
        depends_on_past={{ task.depends_on_past }},
        dag=dag,
    )

    {% for dependency in task.dependencies -%}
    {% if dependency.dag_name == name -%}
    {{ task.task_name }}.set_upstream({{ dependency.task_name }})
    {% else -%}
    wait_for_{{ dependency.task_name }} = ExternalTaskSensor(
        task_id='wait_for_{{ dependency.task_name }}',
        external_dag_id='{{ dependency.dag_name }}',
        external_task_id='{{ dependency.task_name }}',
        dag=dag,
    )
        
    {{ task.task_name }}.set_upstream(wait_for_{{ dependency.task_name }})
    {% endif -%}
    {% endfor -%}
{% endfor %}
